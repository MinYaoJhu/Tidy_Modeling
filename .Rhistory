scale_color_manual(values = c("#7FC97F", "#386CB0")) +
theme_bw() +
theme(legend.position = "top")
#| echo = FALSE,
#| message = FALSE,
#| fig.height = 4,
#| out.width = "70%",
#| fig.cap = "Speed-ups for model tuning versus the number of workers using different delegation schemes. The diagonal black line indicates a linear speedup where the addition of a new worker process has maximal effect.",
#| fig.alt = "Speed-ups for model tuning versus the number of workers using different delegation schemes. The diagonal black line indicates a linear speedup where the addition of a new worker process has maximal effect. The 'everything' scheme shows that the benefits decrease after three or four workers, especially when there is expensive preprocessing. The 'resamples' scheme has almost linear speedups across all tasks."
ggplot(times, aes(x = num_cores, y = speed_up, color = parallel_over, shape = parallel_over)) +
geom_abline(lty = 1) +
geom_point(size = 2) +
geom_line() +
facet_wrap(~ preprocessing) +
coord_obs_pred() +
scale_color_manual(values = c("#7FC97F", "#386CB0")) +
labs(x = "Number of Workers", y = "Speed-up")  +
theme(legend.position = "top")
coef_penalty <- 0.1
spec <- linear_reg(penalty = coef_penalty) %>% set_engine("glmnet")
spec
spec$args$penalty
spec <- linear_reg(penalty = !!coef_penalty) %>% set_engine("glmnet")
spec$args$penalty
mcmc_args <- list(chains = 3, iter = 1000, cores = 3)
linear_reg() %>% set_engine("stan", !!!mcmc_args)
library(stringr)
ch_2_vars <- str_subset(names(cells), "ch_2")
ch_2_vars
# Still uses a reference to global data (~_~;)
recipe(class ~ ., data = cells) %>%
step_spatialsign(all_of(ch_2_vars))
# Inserts the values into the step ヽ(•‿•)ノ
recipe(class ~ ., data = cells) %>%
step_spatialsign(!!!ch_2_vars)
set.seed(1308)
mlp_sfd_race <-
mlp_wflow %>%
tune_race_anova(
cell_folds,
grid = 20,
# Pass in the parameter object to use the appropriate range:
param_info = mlp_param,
metrics = roc_res
)
#| echo = FALSE,
#| message = FALSE,
#| warning = FALSE,
#| fig.height = 5,
#| out.width = "80%",
#| fig.cap = "The racing process for 20 tuning parameters and 10 resamples",
#| fig.alt = "An illustration of the racing process for 20 tuning parameters and 10 resamples. The analysis is conducted at the first, third, and last resample. As the number of resamples increases, the confidence intervals show some model configurations that do not have confidence intervals that overlap with zero. These are excluded from subsequent resamples."
full_att <- attributes(mlp_sfd_race)
library(finetune)
set.seed(1308)
mlp_sfd_race <-
mlp_wflow %>%
tune_race_anova(
cell_folds,
grid = 20,
param_info = mlp_param,
metrics = roc_res,
control = control_race(verbose_elim = TRUE)
)
library(usemodels)
use_xgboost(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +
Latitude + Longitude,
data = ames_train,
# Add comments explaining some of the code:
verbose = TRUE)
library(usemodels)
use_xgboost(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +
Latitude + Longitude,
data = ames_train,
# Add comments explaining some of the code:
verbose = TRUE)
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(finetune)
library(ggforce)
library(stringr)
library(av)
library(lme4)
data(cells)
theme_set(theme_bw())
# All operating systems
library(doParallel)
# Create a cluster object and then register:
cl <- makePSOCKcluster(2)
registerDoParallel(cl)
library(kableExtra)
tidymodels_prefer()
## -----------------------------------------------------------------------------
load("RData/mlp_times.RData")
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(finetune)
library(ggforce)
library(stringr)
library(av)
library(lme4)
data(cells)
theme_set(theme_bw())
# All operating systems
library(doParallel)
# Create a cluster object and then register:
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
library(kableExtra)
tidymodels_prefer()
## -----------------------------------------------------------------------------
load("RData/mlp_times.RData")
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(finetune)
library(ggforce)
library(stringr)
library(av)
library(lme4)
data(cells)
theme_set(theme_bw())
# All operating systems
library(doParallel)
# Create a cluster object and then register:
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
library(kableExtra)
tidymodels_prefer()
## -----------------------------------------------------------------------------
load("RData/mlp_times.RData")
library(tidymodels)
tidymodels_prefer()
mlp_spec <-
mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>%
set_engine("nnet", trace = 0) %>%
set_mode("classification")
mlp_param <- extract_parameter_set_dials(mlp_spec)
mlp_param %>% extract_parameter_dials("hidden_units")
mlp_param %>% extract_parameter_dials("penalty")
mlp_param %>% extract_parameter_dials("epochs")
crossing(
hidden_units = 1:3,
penalty = c(0.0, 0.1),
epochs = c(100, 200)
)
grid_regular(mlp_param, levels = 2)
mlp_param %>%
grid_regular(levels = c(hidden_units = 3, penalty = 2, epochs = 2))
set.seed(1301)
mlp_param %>%
grid_random(size = 1000) %>% # 'size' is the number of combinations
summary()
library(ggforce)
set.seed(1302)
mlp_param %>%
# The 'original = FALSE' option keeps penalty in log10 units
grid_random(size = 20, original = FALSE) %>%
ggplot(aes(x = .panel_x, y = .panel_y)) +
geom_point() +
geom_blank() +
facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) +
labs(title = "Random design with 20 candidates")
#| fig.height = 6,
#| fig.width = 6,
#| echo = FALSE,
#| fig.cap = "Three tuning parameters with 15 points generated at random",
#| fig.alt = "A scatter plot matrix for three tuning parameters with 20 points generated at random. There are significant gaps in the parameter space."
set.seed(1303)
mlp_param %>%
grid_latin_hypercube(size = 20, original = FALSE) %>%
ggplot(aes(x = .panel_x, y = .panel_y)) +
geom_point() +
geom_blank() +
facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) +
labs(title = "Latin Hypercube design with 20 candidates")
#| fig.height = 6,
#| fig.width = 6,
#| echo = FALSE,
#| fig.cap = "Three tuning parameters with 20 points generated using a space-filling design",
#| fig.alt = "A scatter plot matrix for three tuning parameters with 15 points generated using a space-filling design. There are fewer gaps in the parameter space when compared to the random grid."
library(tidymodels)
data(cells)
cells <- cells %>% select(-case)
set.seed(1304)
cell_folds <- vfold_cv(cells)
mlp_rec <-
recipe(class ~ ., data = cells) %>%
step_YeoJohnson(all_numeric_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_pca(all_numeric_predictors(), num_comp = tune()) %>%
step_normalize(all_numeric_predictors())
mlp_wflow <-
workflow() %>%
add_model(mlp_spec) %>%
add_recipe(mlp_rec)
mlp_param <-
mlp_wflow %>%
extract_parameter_set_dials() %>%
update(
epochs = epochs(c(50, 200)),
num_comp = num_comp(c(0, 40))
)
roc_res <- metric_set(roc_auc)
set.seed(1305)
mlp_reg_tune <-
mlp_wflow %>%
tune_grid(
cell_folds,
grid = mlp_param %>% grid_regular(levels = 2),
metrics = roc_res
)
mlp_reg_tune
autoplot(mlp_reg_tune) +
scale_color_viridis_d(direction = -1) +
theme(legend.position = "top")
#| fig.height = 7,
#| echo = FALSE,
#| fig.cap = "The regular grid results",
#| fig.alt = "A line plot of the regular grid results. The x axis shows the number of hidden units and the y axis is the resampled ROC AUC. There are separate lines for the amount of regularization. There are nine panels for three values for the number of PCA components and the number of epochs. On average, the amount of regularization is important where more is better. Also, on average, the increasing the number of hidden units decreases model effectiveness."
show_best(mlp_reg_tune) %>% select(-.estimator)
set.seed(1306)
mlp_sfd_tune <-
mlp_wflow %>%
tune_grid(
cell_folds,
grid = 10,
# Pass in the parameter object to use the appropriate range:
param_info = mlp_param,
metrics = roc_res
)
mlp_sfd_tune
#| echo = FALSE,
#| fig.cap = "The `autoplot()` method results when used with a space-filling design",
#| fig.alt = "The `autoplot()` method results when used with a space-filling design. The trends show decreasing performance with the number of PCA components as well as the number of hidden units."
autoplot(mlp_sfd_tune)
show_best(mlp_sfd_tune) %>% select(-.estimator)
select_best(mlp_reg_tune, metric = "roc_auc")
logistic_param <-
tibble(
num_comp = 0,
epochs = 125,
hidden_units = 1,
penalty = 1
)
final_mlp_wflow <-
mlp_wflow %>%
finalize_workflow(logistic_param)
final_mlp_wflow
final_mlp_fit <-
final_mlp_wflow %>%
fit(cells)
#| echo = FALSE,
#| message = FALSE,
#| fig.width = 5,
#| fig.height = 4,
#| out.width = "50%",
#| fig.cap = "Worker processes when parallel processing matches resamples to a specific worker process",
#| fig.alt = "A diagram of the worker processes when parallel processing matches resamples to a specific worker process. After the preprocess operations are finished, each model fit is executed on the same worker process."
load("extras/parallel_times/resamples_times.RData")
resamples_times %>%
dplyr::rename(operation = label) %>%
ggplot(aes(y = id_alt, x = duration, fill = operation)) +
geom_bar(stat = "identity", color = "black") +
labs(y = NULL, x = "Elapsed Time") +
scale_fill_brewer(palette = "Paired") +
theme(legend.position = "top")
#| echo = FALSE,
#| message = FALSE,
#| fig.height = 7,
#| out.width = "70%",
#| fig.cap = "Worker processes when preprocessing and modeling tasks are distributed to many workers",
#| fig.alt = "A diagram of the worker processes when preprocessing and modeling tasks are distributed to many workers. In this instance, more comprehensive parallelization is used but some preprocessing tasks are repeated across worker processes."
load("extras/parallel_times/everything_times.RData")
repeats <-
everything_times %>%
dplyr::filter(id == "Fold1" & event == "preproc") %>%
nrow()
everything_times <-
everything_times %>%
dplyr::rename(operation = label) %>%
mutate(operation = as.character(operation)) %>%
arrange(pid, id, operation) %>%
select(pid, id, operation, duration)
start_stop <-
everything_times %>%
pivot_wider(
id_cols = c(pid, id),
names_from = "operation",
values_from = "duration"
) %>%
group_by(pid) %>%
mutate(
total = model + preprocess,
.stop_mod = cumsum(total),
prev = dplyr::lag(total, 1),
prev = ifelse(is.na(prev), 0, prev),
.start_pre = cumsum(prev),
.stop_pre = .start_pre + preprocess,
.start_mod = .stop_pre
) %>%
ungroup()  %>%
select(pid, id, .start_pre, .stop_pre, .start_mod, .stop_mod)
starts <-
start_stop %>%
select(pid, id, contains("start")) %>%
pivot_longer(
cols = c(.start_pre, .start_mod),
values_to = ".start"
) %>%
mutate(
operation = ifelse(grepl("mod$", name), "model", "preprocess")
) %>%
select(-name)
stops <-
start_stop %>%
select(pid, id, contains("stop")) %>%
pivot_longer(
cols = c(.stop_pre, .stop_mod),
values_to = ".stop"
) %>%
mutate(
operation = ifelse(grepl("mod$", name), "model", "preprocess")
) %>%
select(-name)
id_offset <- 0.4
start_stop_dat <-
full_join(starts, stops, by = c("pid", "id", "operation")) %>%
mutate(
id_num = as.numeric(factor(id)),
id_start = id_num - id_offset,
id_stop  = id_num + id_offset
)
start_stop_dat %>%
ggplot(aes(y = id_num, x = .start)) +
geom_rect(
aes(
xmin = .start,
xmax = .stop,
ymin = id_start,
ymax = id_stop,
fill = operation
),
color = "black"
) +
facet_wrap(~ pid, nrow = 2) +
labs(y = NULL, x = "Elapsed Time") +
scale_fill_brewer(palette = "Paired") +
scale_y_continuous(breaks = 1:5, labels = paste("Fold", 1:5)) +
theme_bw() +
theme(legend.position = "top", panel.grid.minor = element_blank())
#| echo = FALSE,
#| message = FALSE,
#| fig.height =4,
#| out.width = "70%",
#| fig.cap = "Execution times for model tuning versus the number of workers using different delegation schemes",
#| fig.alt = "Execution times for model tuning versus the number of workers using different delegation schemes."
load("extras/parallel_times/xgb_times.RData")
ggplot(times, aes(x = num_cores, y = elapsed, color = parallel_over, shape = parallel_over)) +
geom_point(size = 2) +
geom_line() +
facet_wrap(~ preprocessing) +
labs(x = "Number of Workers", y = "Execution Time (s)") +
scale_y_log10() +
scale_color_manual(values = c("#7FC97F", "#386CB0")) +
theme_bw() +
theme(legend.position = "top")
#| echo = FALSE,
#| message = FALSE,
#| fig.height = 4,
#| out.width = "70%",
#| fig.cap = "Speed-ups for model tuning versus the number of workers using different delegation schemes. The diagonal black line indicates a linear speedup where the addition of a new worker process has maximal effect.",
#| fig.alt = "Speed-ups for model tuning versus the number of workers using different delegation schemes. The diagonal black line indicates a linear speedup where the addition of a new worker process has maximal effect. The 'everything' scheme shows that the benefits decrease after three or four workers, especially when there is expensive preprocessing. The 'resamples' scheme has almost linear speedups across all tasks."
ggplot(times, aes(x = num_cores, y = speed_up, color = parallel_over, shape = parallel_over)) +
geom_abline(lty = 1) +
geom_point(size = 2) +
geom_line() +
facet_wrap(~ preprocessing) +
coord_obs_pred() +
scale_color_manual(values = c("#7FC97F", "#386CB0")) +
labs(x = "Number of Workers", y = "Speed-up")  +
theme(legend.position = "top")
coef_penalty <- 0.1
spec <- linear_reg(penalty = coef_penalty) %>% set_engine("glmnet")
spec
spec$args$penalty
spec <- linear_reg(penalty = !!coef_penalty) %>% set_engine("glmnet")
spec$args$penalty
mcmc_args <- list(chains = 3, iter = 1000, cores = 3)
linear_reg() %>% set_engine("stan", !!!mcmc_args)
library(stringr)
ch_2_vars <- str_subset(names(cells), "ch_2")
ch_2_vars
# Still uses a reference to global data (~_~;)
recipe(class ~ ., data = cells) %>%
step_spatialsign(all_of(ch_2_vars))
# Inserts the values into the step ヽ(•‿•)ノ
recipe(class ~ ., data = cells) %>%
step_spatialsign(!!!ch_2_vars)
set.seed(1308)
mlp_sfd_race <-
mlp_wflow %>%
tune_race_anova(
cell_folds,
grid = 20,
# Pass in the parameter object to use the appropriate range:
param_info = mlp_param,
metrics = roc_res
)
remaining <-
mlp_sfd_race %>%
collect_metrics() %>%
dplyr::filter(n == 10)
#| echo = FALSE,
#| message = FALSE,
#| warning = FALSE,
#| fig.height = 5,
#| out.width = "80%",
#| fig.cap = "The racing process for 20 tuning parameters and 10 resamples",
#| fig.alt = "An illustration of the racing process for 20 tuning parameters and 10 resamples. The analysis is conducted at the first, third, and last resample. As the number of resamples increases, the confidence intervals show some model configurations that do not have confidence intervals that overlap with zero. These are excluded from subsequent resamples."
full_att <- attributes(mlp_sfd_race)
race_details <- NULL
for(iter in 1:10) {
tmp <- mlp_sfd_race %>% filter(.order <= iter)
tmp_att <- full_att
tmp_att$row.names <- attr(tmp, "row.names")
attributes(tmp) <- tmp_att
if (nrow(show_best(tmp)) == 1) {
break()
}
race_details <-
bind_rows(
race_details,
finetune:::test_parameters_gls(tmp) %>% mutate(iter = iter))
}
race_details <-
race_details %>%
mutate(
lower = ifelse(iter < 3, NA, lower),
upper = ifelse(iter < 3, NA, upper),
pass = ifelse(iter < 3, TRUE, pass),
decision = ifelse(pass, "retain", "discard"),
decision = ifelse(pass & estimate == 0, "best", decision)
)  %>%
mutate(
.config = factor(.config),
.config = reorder(.config, estimate),
decision = factor(decision, levels = c("best", "retain", "discard"))
)
race_cols <- c(best = "blue", retain = "black", discard = "grey")
iter_three <- race_details %>% dplyr::filter(iter == 3)
iter_three %>%
ggplot(aes(x = -estimate, y = .config)) +
geom_vline(xintercept = 0, lty = 2, color = "green") +
geom_point(size = 2, aes(color = decision)) +
geom_errorbarh(aes(xmin = -estimate, xmax = -upper, color = decision), height = .3, show.legend = FALSE) +
labs(x = "Loss of ROC AUC", y = NULL) +
scale_colour_manual(values = race_cols)
race_ci_plots <- function(x, iters = max(x$iter)) {
x_rng <- extendrange(c(-x$estimate, -x$upper))
for (i in 1:iters) {
if (i < 3) {
ttl <- paste0("Iteration ", i, ": burn-in")
} else {
ttl <- paste0("Iteration ", i, ": testing")
}
p <-
x %>%
dplyr::filter(iter == i) %>%
ggplot(aes(x = -estimate, y = .config, color = decision)) +
geom_vline(xintercept = 0, color = "green", lty = 2) +
geom_point(size = 2) +
labs(title = ttl, y = "", x = "Loss of ROC AUC") +
scale_color_manual(values = c(best = "blue", retain = "black", discard = "grey"),
drop = FALSE) +
scale_y_discrete(drop = FALSE) +
xlim(x_rng) +
theme_bw() +
theme(legend.position = "top")
if (i >= 3) {
p <- p  + geom_errorbar(aes(xmin = -estimate, xmax = -upper), width = .3)
}
print(p)
}
invisible(NULL)
}
av_capture_graphics(
race_ci_plots(race_details),
output = "race_results.mp4",
width = 720,
height = 720,
res = 120,
framerate = 1/3
)
show_best(mlp_sfd_race, n = 10)
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(finetune)
library(ggforce)
library(stringr)
library(av)
library(lme4)
data(cells)
theme_set(theme_bw())
# All operating systems
library(doParallel)
# Create a cluster object and then register:
cl <- makePSOCKcluster(8)
registerDoParallel(cl)
library(kableExtra)
tidymodels_prefer()
## -----------------------------------------------------------------------------
load("RData/mlp_times.RData")
