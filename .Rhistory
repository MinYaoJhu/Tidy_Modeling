install.packages("modeldata")
knitr::opts_chunk$set(fig.path = "figures/")
library(tidyverse)
library(gridExtra)
library(tibble)
library(kableExtra)
data(ames, package = "modeldata")
knitr::opts_chunk$set(fig.path = "figures/")
library(tidyverse)
library(gridExtra)
library(tibble)
library(kableExtra)
data(ames, package = "modeldata")
load("RData/plm_resids.RData")
load("RData/plm_resids.RData")
resid_cols <- RColorBrewer::brewer.pal(8, "Set1")[1:2]
# Red is where intensity is higher than expected
plm_plot <-
plm_resids %>%
mutate(sign = ifelse(Intensity < 0, "low", "high")) %>%
ggplot(aes(x = x, y = y, fill = sign))  +
geom_tile(show.legend = FALSE) +
facet_wrap(~Sample) +
theme(
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.text.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks = element_blank()
) +
labs(x = "", y = "") +
scale_fill_manual(values = c("white", "midnightblue")) +
coord_equal() +
ggtitle("(a) Evaluating the quality of two microarray chips using a model.") +
theme(plot.title = element_text(hjust = 0.5))
ames_plot <-
ggplot(ames, aes(x = Latitude, y = Sale_Price)) +
geom_point(alpha = .2) +
geom_smooth(se = FALSE, method = stats::loess, method.args = list(span = .3), color = "red") +
scale_y_log10() +
ylab("House Sale Price ($US)") +
ggtitle("(b) Using a model-based smoother to discover trends.")
grid.arrange(plm_plot, ames_plot, ncol = 1)
knitr::include_graphics("premade/data-science-model.svg")
knitr::include_graphics("premade/data-science-model.svg")
knitr::include_graphics("premade/modeling-process.svg")
knitr::opts_chunk$set(fig.path = "figures/")
library(tidyverse)
library(gridExtra)
library(tibble)
library(kableExtra)
data(ames, package = "modeldata")
load("RData/plm_resids.RData")
resid_cols <- RColorBrewer::brewer.pal(8, "Set1")[1:2]
# Red is where intensity is higher than expected
plm_plot <-
plm_resids %>%
mutate(sign = ifelse(Intensity < 0, "low", "high")) %>%
ggplot(aes(x = x, y = y, fill = sign))  +
geom_tile(show.legend = FALSE) +
facet_wrap(~Sample) +
theme(
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.text.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks = element_blank()
) +
labs(x = "", y = "") +
scale_fill_manual(values = c("white", "midnightblue")) +
coord_equal() +
ggtitle("(a) Evaluating the quality of two microarray chips using a model.") +
theme(plot.title = element_text(hjust = 0.5))
ames_plot <-
ggplot(ames, aes(x = Latitude, y = Sale_Price)) +
geom_point(alpha = .2) +
geom_smooth(se = FALSE, method = stats::loess, method.args = list(span = .3), color = "red") +
scale_y_log10() +
ylab("House Sale Price ($US)") +
ggtitle("(b) Using a model-based smoother to discover trends.")
grid.arrange(plm_plot, ames_plot, ncol = 1)
knitr::include_graphics("premade/data-science-model.svg")
knitr::include_graphics("premade/modeling-process.svg")
monolog <-
tribble(
~Activity, ~`Analysis Cycle`, ~Thoughts,
"EDA", "1",
"The daily ridership values between stations are extremely correlated.",
"EDA", " ",
"Weekday and weekend ridership look very different.",
"EDA", " ",
"One day in the summer of 2010 has an abnormally large number of riders.",
"EDA", "1",
"Which stations had the lowest daily ridership values?",
"Feature Engineering", "1",
"Dates should at least be encoded as day-of-the-week, and year. ",
"Feature Engineering", " ",
"Maybe PCA could be used on the correlated predictors to make it easier for the models to use them. ",
"Feature Engineering", " ",
"Hourly weather records should probably be summarized into daily measurements. ",
"Model Fitting", "1",
"Let’s start with simple linear regression, K-nearest neighbors, and a boosted decision tree. ",
"Model Tuning", "1",
"How many neighbors should be used?",
"Model Tuning", " ",
"Should we run a lot of boosting iterations or just a few?",
"Model Tuning", "2",
"How many neighbors seemed to be optimal for these data? ",
"Model Evaluation", "2",
"Which models have the lowest root mean squared errors? ",
"EDA", "2",
"Which days were poorly predicted? ",
"Model Evaluation", "2",
"Variable importance scores indicate that the weather information is not predictive. We’ll drop them from the next set of models. ",
"Model Evaluation", " ",
"It seems like we should focus on a lot of boosting iterations for that model.",
"Feature Engineering", "2",
"We need to encode holiday features to improve predictions on (and around) those dates.",
"Model Evaluation", "2",
"Let’s drop KNN from the model list. "
)
if (knitr::is_html_output()) {
tab <-
monolog %>%
dplyr::select(Thoughts, Activity) %>%
kable(
caption = "Hypothetical inner monologue of a model developer.",
label = "inner-monologue"
) %>%
kable_styling() %>%
column_spec(2, width = "25%") %>%
column_spec(1, width = "75%", italic = TRUE)
} else {
tab <-
monolog %>%
dplyr::select(Thoughts, Activity) %>%
kable(
caption = "Hypothetical inner monologue of a model developer.",
label = "inner-monologue"
) %>%
kable_styling()
}
tab
knitr::opts_chunk$set(fig.path = "figures/")
library(tidyverse)
library(lubridate)
boot_samp <- rsample::bootstraps(mtcars, times = 3)
boot_samp
class(boot_samp)
n <- nrow(mtcars)
ratios <- rep(NA_real_, n)
for (car in 1:n) {
ratios[car] <- log(mtcars$mpg[car]/mtcars$wt[car])
}
head(ratios)
ratios <- log(mtcars$mpg/mtcars$wt)
compute_log_ratio <- function(mpg, wt) {
log_base <- getOption("log_base", default = exp(1)) # gets external data
results <- log(mpg/wt, base = log_base)
print(mean(results))                                # prints to the console
done <<- TRUE                                       # sets external data
results
}
compute_log_ratio <- function(mpg, wt, log_base = exp(1)) {
log(mpg/wt, base = log_base)
}
map(head(mtcars$mpg, 3), sqrt)
map_dbl(head(mtcars$mpg, 3), sqrt)
log_ratios <- map2_dbl(mtcars$mpg, mtcars$wt, compute_log_ratio)
head(log_ratios)
map2_dbl(mtcars$mpg, mtcars$wt, ~ log(.x/.y)) %>%
head()
# Wants valid names:
data.frame(`variable 1` = 1:2, two = 3:4)
# But can be coerced to use them with an extra option:
df <- data.frame(`variable 1` = 1:2, two = 3:4, check.names = FALSE)
df
# But tibbles just work:
tbbl <- tibble(`variable 1` = 1:2, two = 3:4)
tbbl
df$tw
tbbl$tw
df[, "two"]
tbbl[, "two"]
url <- "chi.csv"
train_cols <-
cols(
station_id = col_double(),
stationname = col_character(),
date = col_character(),
daytype = col_character(),
rides = col_double()
)
num_combos <-
read_delim(url, delim = ",", col_types = train_cols) %>%
distinct(date, stationname) %>%
nrow()
url <- "chi.csv"
train_cols <-
cols(
station_id = col_double(),
stationname = col_character(),
date = col_character(),
daytype = col_character(),
rides = col_double()
)
num_combos <-
read_delim(url, delim = ",", col_types = train_cols) %>%
distinct(date, stationname) %>%
nrow()
library(tidyverse)
library(lubridate)
url <- "https://data.cityofchicago.org/api/views/5neh-572f/rows.csv?accessType=DOWNLOAD&bom=true&format=true"
all_stations <-
# Step 1: Read in the data.
read_csv(url) %>%
# Step 2: filter columns and rename stationname
dplyr::select(station = stationname, date, rides) %>%
# Step 3: Convert the character date field to a date encoding.
# Also, put the data in units of 1K rides
mutate(date = mdy(date), rides = rides / 1000) %>%
# Step 4: Summarize the multiple records using the maximum.
group_by(date, station) %>%
summarize(rides = max(rides), .groups = "drop")
mtcars[order(mtcars$gear, mtcars$mpg), ]
library(dplyr)
arrange(.data = mtcars, gear, mpg)
knitr::opts_chunk$set(fig.path = "figures/")
library(tidyverse)
library(lubridate)
boot_samp <- rsample::bootstraps(mtcars, times = 3)
boot_samp
class(boot_samp)
n <- nrow(mtcars)
ratios <- rep(NA_real_, n)
for (car in 1:n) {
ratios[car] <- log(mtcars$mpg[car]/mtcars$wt[car])
}
head(ratios)
ratios <- log(mtcars$mpg/mtcars$wt)
compute_log_ratio <- function(mpg, wt) {
log_base <- getOption("log_base", default = exp(1)) # gets external data
results <- log(mpg/wt, base = log_base)
print(mean(results))                                # prints to the console
done <<- TRUE                                       # sets external data
results
}
compute_log_ratio <- function(mpg, wt, log_base = exp(1)) {
log(mpg/wt, base = log_base)
}
map(head(mtcars$mpg, 3), sqrt)
map_dbl(head(mtcars$mpg, 3), sqrt)
log_ratios <- map2_dbl(mtcars$mpg, mtcars$wt, compute_log_ratio)
head(log_ratios)
map2_dbl(mtcars$mpg, mtcars$wt, ~ log(.x/.y)) %>%
head()
# Wants valid names:
data.frame(`variable 1` = 1:2, two = 3:4)
# But can be coerced to use them with an extra option:
df <- data.frame(`variable 1` = 1:2, two = 3:4, check.names = FALSE)
df
# But tibbles just work:
tbbl <- tibble(`variable 1` = 1:2, two = 3:4)
tbbl
df$tw
tbbl$tw
df[, "two"]
tbbl[, "two"]
url <- "chi.csv"
train_cols <-
cols(
station_id = col_double(),
stationname = col_character(),
date = col_character(),
daytype = col_character(),
rides = col_double()
)
num_combos <-
read_delim(url, delim = ",", col_types = train_cols) %>%
distinct(date, stationname) %>%
nrow()
library(tidyverse)
library(lubridate)
url <- "https://data.cityofchicago.org/api/views/5neh-572f/rows.csv?accessType=DOWNLOAD&bom=true&format=true"
all_stations <-
# Step 1: Read in the data.
read_csv(url) %>%
# Step 2: filter columns and rename stationname
dplyr::select(station = stationname, date, rides) %>%
# Step 3: Convert the character date field to a date encoding.
# Also, put the data in units of 1K rides
mutate(date = mdy(date), rides = rides / 1000) %>%
# Step 4: Summarize the multiple records using the maximum.
group_by(date, station) %>%
summarize(rides = max(rides), .groups = "drop")
knitr::opts_chunk$set(fig.path = "figures/")
data(crickets, package = "modeldata")
library(tidyverse)
library(kableExtra)
#| out.width = '70%',
#| fig.width = 6,
#| fig.height = 4,
#| warning = FALSE,
#| message = FALSE,
#| echo = FALSE,
#| fig.cap = "Relationship between chirp rate and temperature for two different species of crickets",
#| fig.alt = "A scatter plot of the chirp rate and temperature for two different species of crickets with linear trend lines per species. The trends are linearly increasing with a separation between the two species."
rate ~ temp
rate ~ temp + time
rate ~ temp + species
rate ~ temp + species + temp:species
# A shortcut can be used to expand all interactions containing
# interactions with two variables:
rate ~ (temp + species)^2
# Another shortcut to expand factors to include all possible
# interactions (equivalent for this example):
rate ~ temp * species
interaction_fit <-  lm(rate ~ (temp + species)^2, data = crickets)
# To print a short summary of the model:
interaction_fit
#| out.width = '100%',
#| fig.width = 8,
#| fig.height = 4.5,
#| warning = FALSE,
#| echo = FALSE,
#| fig.cap = "Residual diagnostic plots for the linear model with interactions, which appear reasonable enough to conduct inferential analysis",
#| fig.alt = "On the left is a scatter plot of the model residuals versus predicted values. There are no strong trends in the data. The right-hand panel shows a normal quantile-quantile plot where the points indicate that normality is probably a good assumption."
# Fit a reduced model:
main_effect_fit <-  lm(rate ~ temp + species, data = crickets)
# Compare the two:
anova(main_effect_fit, interaction_fit)
summary(main_effect_fit)
new_values <- data.frame(species = "O. exclamationis", temp = 15:20)
predict(main_effect_fit, new_values)
prob_tbl <-
tribble(
~ Function, ~Package, ~Code,
"lda()"        , "MASS"       ,  "predict(object)"                      ,
"glm()"        , "stats"      ,  'predict(object, type = "response")'          ,
"gbm()"        , "gbm"        ,  'predict(object, type = "response", n.trees)' ,
"mda()"        , "mda"        ,  'predict(object, type = "posterior")'         ,
"rpart()"      , "rpart"      ,  'predict(object, type = "prob")'              ,
"various"      , "RWeka"      ,  'predict(object, type = "probability")'       ,
"logitboost()" , "LogitBoost" ,  'predict(object, type = "raw", nIter)'        ,
"pamr.train()" , "pamr"       ,  'pamr.predict(object, type = "posterior")'
)
prob_tbl %>%
kable(
caption = "Heterogeneous argument names for different modeling functions.",
label = "probability-args",
escape = FALSE
) %>%
kable_styling(full_width = FALSE) %>%
column_spec(1, monospace = ifelse(prob_tbl$Function == "various", FALSE, TRUE)) %>%
column_spec(3, monospace = TRUE)
# Add a missing value to the prediction set
new_values$temp[1] <- NA
# The predict method for `lm` defaults to `na.pass`:
predict(main_effect_fit, new_values)
# Alternatively
predict(main_effect_fit, new_values, na.action = na.fail)
predict(main_effect_fit, new_values, na.action = na.omit)
corr_res <- map(mtcars %>% select(-mpg), cor.test, y = mtcars$mpg)
# The first of ten results in the vector:
corr_res[[1]]
library(broom)
tidy(corr_res[[1]])
#| echo = FALSE,
#| fig.cap = "Correlations (and 95% confidence intervals) between predictors and the outcome in the `mtcars` data set",
#| fig.alt = "A plot of the correlations (and 95% confidence intervals) between predictors and the outcome in the `mtcars` data set. None of the intervals overlap with zero. The car weight had the largest negative correlation and the rear axle ratio has the highest positive correlation."
split_by_species <-
crickets %>%
group_nest(species)
split_by_species
model_by_species <-
split_by_species %>%
mutate(model = map(data, ~ lm(rate ~ temp, data = .x)))
model_by_species
model_by_species %>%
mutate(coef = map(model, tidy)) %>%
select(species, coef) %>%
unnest(cols = c(coef))
pkgs <- paste0("package:",
c("kableExtra",
"tidyverse", "tidymodels",
tidyverse:::core, tidymodels:::core))
for (i in pkgs) {
try(detach(i, unload = TRUE, character.only = TRUE, force = TRUE), silent = TRUE)
}
library(tidymodels)
tidymodels_prefer(quiet = FALSE)
plot(plot_data$x, plot_data$y)
