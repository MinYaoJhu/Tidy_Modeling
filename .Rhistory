geom_histogram(bins = 30, color = "white", fill = "blue", alpha = 1/3) +
ggtitle("Original validation set data")
p2 <-
bean_val_processed %>%
ggplot(aes(x = area)) +
geom_histogram(bins = 30, color = "white", fill = "red", alpha = 1/3) +
ggtitle("Processed validation set data")
p1 + p2
#| echo = FALSE,
#| fig.height=4,
#| fig.cap = "The `area` predictor before and after preprocessing",
#| fig.alt = "The `area` predictor before and after preprocessing. The before panel shows a right-skewed, slightly bimodal distribution. The after panel has a distribution that is fairly bell shaped."
bake(bean_rec_trained, new_data = NULL) %>% nrow()
bean_train %>% nrow()
library(ggforce)
plot_validation_results <- function(recipe, dat = bean_validation) {
recipe %>%
# Estimate any additional steps
prep() %>%
# Process the data (the validation set by default)
bake(new_data = dat) %>%
# Create the scatterplot matrix
ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +
geom_point(alpha = 0.4, size = 0.5) +
geom_autodensity(alpha = .3) +
facet_matrix(vars(-class), layer.diag = 2) +
scale_color_brewer(palette = "Dark2") +
scale_fill_brewer(palette = "Dark2")
}
bean_rec_trained %>%
step_pca(all_numeric_predictors(), num_comp = 4) %>%
plot_validation_results() +
ggtitle("Principal Component Analysis")
#| dev = "png",
#| echo = FALSE,
#| fig.height = 7,
#| fig.cap = "Principal component scores for the bean validation set, colored by class",
#| fig.alt = "Principal component scores for the bean validation set, colored by class. The classes separate when the first two components are plotted against one another."
library(learntidymodels)
bean_rec_trained %>%
step_pca(all_numeric_predictors(), num_comp = 4) %>%
prep() %>%
plot_top_loadings(component_number <= 4, n = 5) +
scale_fill_brewer(palette = "Paired") +
ggtitle("Principal Component Analysis")
#| echo = FALSE,
#| fig.cap = "Predictor loadings for the PCA transformation",
#| fig.alt = "Predictor loadings for the PCA transformation. For the first component, the major axis length, second shape factor, convex area, and area have the largest effect. "
bean_rec_trained %>%
step_pls(all_numeric_predictors(), outcome = "class", num_comp = 4) %>%
plot_validation_results() +
ggtitle("Partial Least Squares")
#| dev = "png",
#| fig.height = 7,
#| echo = FALSE,
#| fig.cap = "PLS component scores for the bean validation set, colored by class",
#| fig.alt = "PLS component scores for the bean validation set, colored by class. The first two PLS components are nearly identical to the first two PCA components."
bean_rec_trained %>%
step_pls(all_numeric_predictors(), outcome = "class", num_comp = 4) %>%
prep() %>%
plot_top_loadings(component_number <= 4, n = 5, type = "pls") +
scale_fill_brewer(palette = "Paired") +
ggtitle("Partial Least Squares")
#| echo = FALSE,
#| fig.cap = "Predictor loadings for the PLS transformation",
#| fig.alt = "Predictor loadings for the PLS transformation. For the first component, the major axis length, second shape factor, the equivalent diameter, convex area, and area have the largest effect. "
bean_rec_trained %>%
step_ica(all_numeric_predictors(), num_comp = 4) %>%
plot_validation_results() +
ggtitle("Independent Component Analysis")
#| dev = "png",
#| echo = FALSE,
#| fig.height = 7,
#| fig.cap = "ICA component scores for the bean validation set, colored by class",
#| fig.alt = "ICA component scores for the bean validation set, colored by class. There is significant overlap in the first two ICA components."
library(embed)
bean_rec_trained %>%
step_umap(all_numeric_predictors(), num_comp = 4) %>%
plot_validation_results() +
ggtitle("UMAP")
#| dev = "png",
#| echo = FALSE,
#| fig.height = 7,
#| fig.cap = "UMAP component scores for the bean validation set, colored by class",
#| fig.alt = "UMAP component scores for the bean validation set, colored by class. There is a very high degree of separation between clusters, but several of the clusters contain more than one class."
bean_rec_trained %>%
step_umap(all_numeric_predictors(), outcome = "class", num_comp = 4) %>%
plot_validation_results() +
ggtitle("UMAP (supervised)")
#| dev = "png",
#| echo = FALSE,
#| fig.height = 7,
#| fig.cap = "Supervised UMAP component scores for the bean validation set, colored by class",
#| fig.alt = "Supervised UMAP component scores for the bean validation set, colored by class. There is again a very high degree of separation between clusters, and there are now fewer instances of one cluster containing multiple classes."
library(baguette)
library(discrim)
library(earth)
library(mda)
mlp_spec <-
mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>%
set_engine('nnet') %>%
set_mode('classification')
bagging_spec <-
bag_tree() %>%
set_engine('rpart') %>%
set_mode('classification')
fda_spec <-
discrim_flexible(
prod_degree = tune()
) %>%
set_engine('earth')
rda_spec <-
discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %>%
set_engine('klaR')
bayes_spec <-
naive_Bayes() %>%
set_engine('klaR')
bean_rec <-
recipe(class ~ ., data = bean_train) %>%
step_zv(all_numeric_predictors()) %>%
step_orderNorm(all_numeric_predictors()) %>%
step_normalize(all_numeric_predictors())
pls_rec <-
bean_rec %>%
step_pls(all_numeric_predictors(), outcome = "class", num_comp = tune())
umap_rec <-
bean_rec %>%
step_umap(
all_numeric_predictors(),
outcome = "class",
num_comp = tune(),
neighbors = tune(),
min_dist = tune()
)
ctrl <- control_grid(parallel_over = "everything")
bean_res <-
workflow_set(
preproc = list(basic = class ~., pls = pls_rec, umap = umap_rec),
models = list(bayes = bayes_spec, fda = fda_spec,
rda = rda_spec, bag = bagging_spec,
mlp = mlp_spec)
) %>%
workflow_map(
verbose = TRUE,
seed = 1603,
resamples = bean_val,
grid = 10,
metrics = metric_set(roc_auc),
control = ctrl
)
rankings <-
rank_results(bean_res, select_best = TRUE) %>%
mutate(method = map_chr(wflow_id, ~ str_split(.x, "_", simplify = TRUE)[1]))
tidymodels_prefer()
filter(rankings, rank <= 5) %>% dplyr::select(rank, mean, model, method)
#| echo = FALSE,
#| fig.cap = "Area under the ROC curve from the validation set",
#| fig.alt = "Area under the ROC curve from the validation set. The three best model configurations use PLS together with regularized discriminant analysis, a multi-layer perceptron, and a naive Bayes model."
rankings %>%
ggplot(aes(x = rank, y = mean, pch = method, color = model)) +
geom_point(cex = 3.5) +
theme(legend.position = "right") +
labs(y = "ROC AUC")  +
geom_text(aes(y = mean - 0.01, label = wflow_id), angle = 90, hjust = 1) +
lims(y = c(0.9, NA))
rda_res <-
bean_res %>%
extract_workflow("pls_rda") %>%
finalize_workflow(
bean_res %>%
extract_workflow_set_result("pls_rda") %>%
select_best(metric = "roc_auc")
) %>%
last_fit(split = bean_split, metrics = metric_set(roc_auc))
rda_wflow_fit <- extract_workflow(rda_res)
collect_metrics(rda_res)
save(rda_wflow_fit, bean_train, file = "RData/rda_fit.RData", version = 2, compress = "xz")
install.packages("text2vec")
install.packages(c("ape", "bookdown", "brio", "broom.mixed", "butcher", "censored", "data.table", "DT", "effectsize", "emmeans", "fs", "ggeffects", "ggplot2", "gtable", "htmltools", "httr2", "knitr", "labelled", "lme4", "matrixStats", "munsell", "opdisDownsampling", "openssl", "pkgdown", "promises", "RcppArmadillo", "renv", "reticulate", "segmented", "seriation", "shiny", "StanHeaders", "styler", "survival", "testthat", "tune", "uwot", "xopen"))
install.packages(c("ape", "bookdown", "brio", "broom.mixed", "butcher", "censored", "data.table", "DT", "effectsize", "emmeans", "fs", "ggeffects", "ggplot2", "gtable", "htmltools", "httr2", "knitr", "labelled", "lme4", "matrixStats", "munsell", "opdisDownsampling", "openssl", "pkgdown", "promises", "RcppArmadillo", "renv", "reticulate", "segmented", "seriation", "shiny", "StanHeaders", "styler", "survival", "testthat", "tune", "uwot", "xopen"))
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(applicable)
library(patchwork)
library(probably)
load("RData/Chicago_2020.RData")
data(Chicago)
library(tidymodels)
tidymodels_prefer()
simulate_two_classes <-
function (n, error = 0.1, eqn = quote(-1 - 2 * x - 0.2 * x^2 + 2 * y^2))  {
# Slightly correlated predictors
sigma <- matrix(c(1, 0.7, 0.7, 1), nrow = 2, ncol = 2)
dat <- MASS::mvrnorm(n = n, mu = c(0, 0), Sigma = sigma)
colnames(dat) <- c("x", "y")
cls <- paste0("class_", 1:2)
dat <-
as_tibble(dat) %>%
mutate(
linear_pred = !!eqn,
# Add some misclassification noise
linear_pred = linear_pred + rnorm(n, sd = error),
prob = binomial()$linkinv(linear_pred),
class = ifelse(prob > runif(n), cls[1], cls[2]),
class = factor(class, levels = cls)
)
dplyr::select(dat, x, y, class)
}
set.seed(1901)
training_set <- simulate_two_classes(200)
testing_set  <- simulate_two_classes(50)
two_class_mod <-
logistic_reg() %>%
set_engine("stan", seed = 1902) %>%
fit(class ~ . + I(x^2)+ I(y^2), data = training_set)
print(two_class_mod, digits = 3)
#| echo = FALSE,
#| fig.width=6,
#| fig.height=6,
#| out.width="70%",
#| fig.align="center",
#| fig.cap = "Simulated two-class data set with a logistic regression fit and decision boundary.",
#| fig.alt = "Simulated two-class data set with a logistic regression fit and decision boundary. The scatter plot of the two classes shows fairly correlated data. The decision boundary is a parabola in the x axis that does a good job of separating the classes."
data_grid <-
crossing(
x = seq(-4.5, 4.5, length = 100),
y = seq(-4.5, 4.5, length = 100)
)
grid_pred <-
predict(two_class_mod, data_grid, type = "prob") %>%
bind_cols(
predict(two_class_mod, data_grid, type = "pred_int", std_error = TRUE),
data_grid
)
grid_pred %>%
mutate(`Probability of Class 1` = .pred_class_1) %>%
ggplot(aes(x = x, y = y)) +
geom_raster(aes(fill = `Probability of Class 1`)) +
geom_point(data = testing_set, aes(shape = class, color = class), alpha = .75, size = 2.5) +
geom_contour(aes(z = .pred_class_1), breaks = .5, color = "black", lty = 2) +
coord_equal() +
labs(x = "Predictor x", y = "Predictor y", color = NULL, shape = NULL) +
scale_fill_gradient2(low = "#FDB863", mid = "white", high = "#B2ABD2", midpoint = .5) +
scale_color_manual(values = c("#2D004B", "darkorange"))
test_pred <- augment(two_class_mod, testing_set)
test_pred %>% head()
library(probably)
lvls <- levels(training_set$class)
test_pred <-
test_pred %>%
mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = 0.15))
test_pred %>% count(.pred_with_eqz)
# All data
test_pred %>% conf_mat(class, .pred_class)
# Reportable results only:
test_pred %>% conf_mat(class, .pred_with_eqz)
#| echo = FALSE,
#| out.width="80%",
#| fig.cap = "The effect of equivocal zones on model performance",
#| fig.alt = "The effect of equivocal zones on model performance. There is a slight increase in accuracy at the expense of a falling reportable rate."
test_pred <-
test_pred %>%
bind_cols(
predict(two_class_mod, testing_set, type = "pred_int", std_error = TRUE)
)
#| echo = FALSE,
#| fig.width=5,
#| fig.height=5,
#| out.width="70%",
#| fig.align="center",
#| fig.cap = "The effect of the standard error of prediction overlaid with the test set data",
#| fig.alt = "The effect of the standard error of prediction overlaid with the test set data. The region of large variation is very similar to the class boundary space. Additionally, there is a large amount of variation to the west of the inflection point of the boundary curve."
grid_pred %>%
mutate(`Std Error` = .std_error) %>%
ggplot(aes(x = x, y = y)) +
geom_raster(aes(fill = `Std Error`)) +
scale_fill_gradientn(colours = c("#F7FBFF", "#DEEBF7", "#C6DBEF", "#9ECAE1", "#6BAED6")) +
geom_point(data = testing_set, aes(shape = class), alpha = .5, size = 2) +
coord_equal() +
labs(x = "Predictor x", y = "Predictor y", shape = NULL)
## loads both `Chicago` data set as well as `stations`
data(Chicago)
Chicago <- Chicago %>% select(ridership, date, one_of(stations))
n <- nrow(Chicago)
Chicago_train <- Chicago %>% slice(1:(n - 14))
Chicago_test  <- Chicago %>% slice((n - 13):n)
base_recipe <-
recipe(ridership ~ ., data = Chicago_train) %>%
# Create date features
step_date(date) %>%
step_holiday(date, keep_original_cols = FALSE) %>%
# Create dummy variables from factor columns
step_dummy(all_nominal()) %>%
# Remove any columns with a single unique value
step_zv(all_predictors()) %>%
step_normalize(!!!stations)%>%
step_pls(!!!stations, num_comp = 10, outcome = vars(ridership))
lm_spec <-
linear_reg() %>%
set_engine("lm")
lm_wflow <-
workflow() %>%
add_recipe(base_recipe) %>%
add_model(lm_spec)
set.seed(1902)
lm_fit <- fit(lm_wflow, data = Chicago_train)
res_test <-
predict(lm_fit, Chicago_test) %>%
bind_cols(
predict(lm_fit, Chicago_test, type = "pred_int"),
Chicago_test
)
res_test %>% select(date, ridership, starts_with(".pred"))
res_test %>% rmse(ridership, .pred)
#| echo = FALSE,
#| fig.height=4,
#| out.width="80%",
#| fig.align="center",
#| fig.cap = "Two weeks of 2016 predictions for the Chicago data along with 95% prediction intervals",
#| fig.alt = "Two weeks of 2016 predictions for the Chicago data along with 95% prediction intervals. The model fit the data fairly well with reasonable error estimates."
add_day <- function(x) {
day <- lubridate::wday(x$date, label = TRUE)
factor(as.character(day), ordered = FALSE, levels = levels(day))
}
res_test %>%
mutate(day = add_day(.)) %>%
ggplot(aes(x = date)) +
geom_point(aes(y = ridership, color = day, pch = day), size = 3) +
geom_line(aes(y = .pred), alpha = .75) +
geom_ribbon(aes(ymin = .pred_lower, ymax = .pred_upper), fill = "blue", alpha = .1)  +
scale_color_brewer(palette = "Set2") +
scale_shape_manual(values = 15:22) +
scale_x_date(labels = date_format("%B %d, %Y")) +
labs(x = NULL, y = "Daily Ridership (x1000)", color = NULL, pch = NULL)
res_2020 <-
predict(lm_fit, Chicago_2020) %>%
bind_cols(
predict(lm_fit, Chicago_2020, type = "pred_int"),
Chicago_2020
)
res_2020 %>% select(date, contains(".pred"))
res_2020 %>% select(date, ridership, starts_with(".pred"))
res_2020 %>% rmse(ridership, .pred)
#| echo = FALSE,
#| fig.height=4,
#| out.width="80%",
#| fig.align="center",
#| fig.cap = "Two weeks of 2020 predictions for the Chicago data along with 95% prediction intervals",
#| fig.alt = "Two weeks of 2016 predictions for the Chicago data along with 95% prediction intervals. The model fit the data fairly well with reasonable error estimates."
res_2020 %>%
mutate(day = add_day(.)) %>%
ggplot(aes(x = date)) +
geom_point(aes(y = ridership, color = day, pch = day), size = 3) +
geom_line(aes(y = .pred), alpha = .75) +
geom_ribbon(aes(ymin = .pred_lower, ymax = .pred_upper), fill = "blue", alpha = .1) +
scale_shape_manual(values = 15:22) +
scale_color_brewer(palette = "Set2") +
scale_x_date(labels = date_format("%B %d, %Y")) +
labs(x = NULL, y = "Daily Ridership (x1000)", color = NULL, pch = NULL)
#| echo = FALSE,
#| out.width = "100%",
#| fig.cap = "The PCA reference distribution based on the training set",
#| fig.alt = "The PCA reference distribution based on the training set. The majority of the distances to the center of the PCA distribution are below a value of three."
pca_rec <- recipe(~ ., data = Chicago_train) %>%
step_normalize(California, Austin) %>%
step_pca(California, Austin, num_comp = 2) %>%
prep()
training_pca <- bake(pca_rec, new_data = NULL)
pca_center <-
training_pca %>%
select(PC1, PC2) %>%
summarize(PC1_mean = mean(PC1), PC2_mean = mean(PC2))
training_pca <-
cbind(pca_center, training_pca) %>%
mutate(
distance = (PC1 - PC1_mean)^2 + (PC2 - PC2_mean)^2,
distance = sqrt(distance)
)
testing_pca <-
bake(pca_rec, Chicago_test %>% slice(1)) %>%
cbind(pca_center) %>%
mutate(
distance = (PC1 - PC1_mean)^2 + (PC2 - PC2_mean)^2,
distance = sqrt(distance)
)
testing_pctl <- round(mean(training_pca$distance <= testing_pca$distance) * 100, 1)
new_pca <-
bake(pca_rec, Chicago_2020 %>% slice(6)) %>%
cbind(pca_center) %>%
mutate(
distance = (PC1 - PC1_mean)^2 + (PC2 - PC2_mean)^2,
distance = sqrt(distance)
)
new_pctl <- round(mean(training_pca$distance <= new_pca$distance) * 100, 1)
tr_plot <-
Chicago_train %>%
ggplot(aes(x = California, y = Austin)) +
geom_point(alpha = .25, size = .3) +
# coord_equal() +
labs(title = "(a) Training Set") +
theme(plot.title = element_text(size=9))
pca_plot <- training_pca %>%
ggplot(aes(x = PC1, y = PC2)) +
geom_point(alpha = .25, size = .3) +
coord_obs_pred() +
labs(x = "Component 1", y = "Component 2", title = "(b) Training Set PCA Scores") +
theme(plot.title = element_text(size = 9))
pca_dist <-
training_pca %>%
ggplot() +
geom_segment(aes(x = PC1_mean, y = PC2_mean,
xend = PC1, yend = PC2), alpha = .1)  +
coord_obs_pred() +
labs(x = "Component 1", y = "Component 2", title = "(c) Distances to Center") +
theme(plot.title = element_text(size = 9))
dist_hist <-
training_pca %>%
ggplot(aes(x = distance)) +
geom_histogram(bins = 30, color = "white") +
labs(x = "Distance to Training Set Center", title = "(d) Reference Distribution") +
theme(plot.title = element_text(size = 9))
library(patchwork)
tr_plot + pca_plot + pca_dist + dist_hist
#| echo = FALSE,
#| out.width = "100%",
#| fig.width=9,
#| fig.height=4,
#| fig.cap = "The reference distribution with two new points: one using the test set and one from the 2020 data",
#| fig.alt = "The reference distribution with two new points: one using the test set and one from the 2020 data. The test set point is snugly within the data mainstream while the 2020 point is outside of the reference distribution."
test_pca_dist <-
training_pca %>%
ggplot() +
geom_segment(
aes(x = PC1_mean, y = PC2_mean, xend = PC1, yend = PC2),
alpha = .05
)  +
geom_segment(
data = testing_pca,
aes(x = PC1_mean, y = PC2_mean, xend = PC1, yend = PC2),
color = "lightblue",
lty = 2
)  +
geom_segment(
data = new_pca,
aes(x = PC1_mean, y = PC2_mean, xend = PC1, yend = PC2),
color = "red"
)  +
geom_point(data = testing_pca, aes(x = PC1, y = PC2), color = "lightblue", size = 2, pch = 17) +
geom_point(data = new_pca, aes(x = PC1, y = PC2), size = 2, color = "red") +
coord_obs_pred() +
labs(x = "Component 1", y = "Component 2", title = "Distances to Training Set Center") +
theme_bw() +
theme(legend.position = "top")
test_dist_hist <-
training_pca %>%
ggplot(aes(x = distance)) +
geom_histogram(bins = 30, color = "white", alpha = .5) +
geom_vline(xintercept = testing_pca$distance, color = "lightblue", lty = 2) +
geom_vline(xintercept = new_pca$distance, color = "red") +
xlab("Distance to Training Set Center")
test_pca_dist + test_dist_hist
library(applicable)
pca_stat <- apd_pca(~ ., data = Chicago_train %>% select(one_of(stations)),
threshold = 0.99)
pca_stat
#| echo = FALSE,
#| out.width="70%",
#| fig.cap = "The results of using the `autoplot()` method on an applicable object",
#| fig.alt = "The results of using the `autoplot()` method on an applicable object."
score(pca_stat, Chicago_test) %>% select(starts_with("distance"))
score(pca_stat, Chicago_2020) %>% select(starts_with("distance"))
install.packages("lime")
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(rules)
library(baguette)
library(stacks)
library(patchwork)
library(kableExtra)
load("RData/concrete_results.RData")
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(rules)
library(baguette)
library(stacks)
library(patchwork)
library(kableExtra)
load("RData/concrete_results.RData")
stacks() %>%
add_candidates(grid_results) %>%
as_tibble() %>%
mutate(
sample_num = row_number(),
buffer_1 = "",
buffer_2 = "") %>%
slice_head(n = 6) %>%
select(sample_num, CART_bagged_1_1, starts_with("MARS"), Cubist_1_01,
buffer_1, Cubist_1_18, buffer_2) %>%
knitr::kable(
digits = 2,
align = rep("c", 8),
col.names = c("Sample #", "Bagged Tree", "MARS 1", "MARS 2", "Cubist 1",
"...", "Cubist 25", "..."),
caption = "Predictions from candidate tuning parameter configurations.",
label = "ensemble-candidate-preds"
) %>%
kable_styling("striped", full_width = TRUE) %>%
add_header_above(c(" ", "Ensemble Candidate Predictions" = 7)) %>%
row_spec(0, align = "c")
View(grid_results)
