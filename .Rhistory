add_role(Austin:California, new_role = "station") %>%
add_role(temp_min:weather_storm, new_role = "weather") %>%
step_normalize(has_role("station")) %>%
step_normalize(has_role("weather")) %>%
step_pca(has_role("station"), threshold = .75, prefix = "station_PC", id = "station_pca") %>%
step_pca(has_role("weather"), threshold = .75, prefix = "weather_PC", id = "weather_pca")
Chicago_rec <-
recipe(ridership ~ ., data = Chicago_train) %>%
update_role(Austin:California, new_role = "station") %>%
update_role(temp_min:weather_storm, new_role = "weather") %>%
step_normalize(has_role("station")) %>%
step_normalize(has_role("weather")) %>%
step_pca(has_role("station"), threshold = .75, prefix = "station_PC", id = "station_pca") %>%
step_pca(has_role("weather"), threshold = .75, prefix = "weather_PC", id = "weather_pca")
Chicago_rec
tidy(Chicago_rec)
lm_wflow <-
workflow() %>%
add_model(lm_model) %>%
add_recipe(Chicago_rec)
lm_fit <- fit(lm_wflow, Chicago_train)
tidy(lm_fit)
tidy(lm_fit) %>% select(term, p.value)
str(Chicago_test)
Chicago_test_nor <- Chicago_test %>% select(-ridership)
str(Chicago_test_nor)
Chicago_test_onlyr <- Chicago_test %>% select(ridership,weekday)
str(Chicago_test_onlyr)
predicted_r <- predict(lm_fit, Chicago_test_nor)
predicted_r
chicago_test_predictions <- bind_cols(predicted_r, Chicago_test_onlyr)
chicago_test_predictions
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred, col = weekday)) +
geom_point(alpha = 0.5) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
geom_smooth(method = lm, formula = y ~ x, color = "blue") +
coord_equal()
set.seed(123) # For reproducibility
cv_folds <- vfold_cv(Chicago_train, v = 10)
# Linear Model Specification
lm_spec <- linear_reg() %>%
set_engine("lm")
# Random Forest Specification
rf_spec <- rand_forest(trees = 1000) %>%
set_engine("ranger") %>%
set_mode("regression")
# Create workflows
lm_workflow <-
workflow() %>%
add_model(lm_spec) %>%
add_recipe(Chicago_rec)
rf_workflow <-
workflow() %>%
add_model(rf_spec) %>%
add_recipe(Chicago_rec)
# Compare models using cross-validation
keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)
set.seed(1003)
lm_res <-
lm_workflow %>%
fit_resamples(resamples = cv_folds, control = keep_pred)
rf_res <-
rf_workflow %>%
fit_resamples(resamples = cv_folds, control = keep_pred)
collect_metrics(lm_res)
collect_metrics(rf_res)
Chicago_rec1 <-
recipe(ridership ~ ., data = Chicago_train) %>%
add_role(Blackhawks_Away:Cubs_Home, new_role = "sports_team") %>%
add_role(Austin:California, new_role = "station") %>%
add_role(temp_min:weather_storm, new_role = "weather") %>%
step_normalize(has_role("station")) %>%
step_normalize(has_role("weather")) %>%
step_pca(has_role("station"), threshold = .75, prefix = "station_PC", id = "station_pca") %>%
step_pca(has_role("weather"), threshold = .75, prefix = "weather_PC", id = "weather_pca")
Chicago_rec1
str(Chicago_train)
Chicago_rec2 <-
recipe(ridership ~ ., data = Chicago_train) %>%
update_role(Blackhawks_Away:Cubs_Home, new_role = "sports_team") %>%
add_role(Austin:California, new_role = "station") %>%
add_role(temp_min:weather_storm, new_role = "weather") %>%
step_normalize(has_role("station")) %>%
step_normalize(has_role("weather")) %>%
step_pca(has_role("station"), threshold = .75, prefix = "station_PC", id = "station_pca") %>%
step_pca(has_role("weather"), threshold = .75, prefix = "weather_PC", id = "weather_pca")
Chicago_rec2
Chicago_rec3 <-
recipe(ridership ~ ., data = Chicago_train) %>%
update_role(Blackhawks_Away:Cubs_Home, new_role = "sports_team") %>%
add_role(Austin:California, new_role = "station") %>%
add_role(temp_min:weather_storm, new_role = "weather") %>%
step_normalize(has_role("station")) %>%
step_normalize(has_role("weather")) %>%
step_pca(has_role("station"), num_comp = 1, prefix = "station_PC", id = "station_pca") %>%
step_pca(has_role("weather"), num_comp = 1, prefix = "weather_PC", id = "weather_pca")
Chicago_rec3
Chicago_rec4 <-
recipe(ridership ~ ., data = Chicago_train) %>%
update_role(Blackhawks_Away:Cubs_Home, new_role = "sports_team") %>%
update_role(Austin:California, new_role = "station") %>%
add_role(temp_min:weather_storm, new_role = "weather") %>%
step_normalize(has_role("weather")) %>%
step_pca(has_role("weather"), num_comp = 1, prefix = "weather_PC", id = "weather_pca")
Chicago_rec4
Chicago_rec5 <-
recipe(ridership ~ ., data = Chicago_train) %>%
update_role(Blackhawks_Away:Cubs_Home, new_role = "sports_team") %>%
add_role(Austin:California, new_role = "station") %>%
update_role(temp_min:weather_storm, new_role = "weather") %>%
step_normalize(has_role("station")) %>%
step_pca(has_role("station"), num_comp = 1, prefix = "station_PC", id = "station_pca")
Chicago_rec5
Chicago_rec6 <-
recipe(ridership ~ ., data = Chicago_train) %>%
update_role(Blackhawks_Away:Cubs_Home, new_role = "sports_team") %>%
update_role(Austin:California, new_role = "station") %>%
update_role(temp_min:weather_storm, new_role = "weather")
Chicago_rec6
library(tidymodels)
tidymodels_prefer()
preproc <-
list(model1 = Chicago_rec1,
model2 = Chicago_rec2,
model3 = Chicago_rec3,
model4 = Chicago_rec4,
model5 = Chicago_rec5,
model6 = Chicago_rec6
)
lm_models <- workflow_set(preproc, list(lm = linear_reg()), cross = FALSE)
lm_models
lm_models <-
lm_models %>%
workflow_map("fit_resamples",
# Options to `workflow_map()`:
seed = 1101, verbose = TRUE,
# Options to `fit_resamples()`:
resamples = cv_folds, control = keep_pred)
lm_models
collect_metrics(lm_models) %>%
filter(.metric == "rmse")
library(ggrepel)
autoplot(lm_models, metric = "rsq") +
geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +
theme(legend.position = "none")
rsq_indiv_estimates <-
collect_metrics(lm_models, summarize = FALSE) %>%
filter(.metric == "rsq")
rsq_wider <-
rsq_indiv_estimates %>%
select(wflow_id, .estimate, id) %>%
pivot_wider(id_cols = "id", names_from = "wflow_id", values_from = ".estimate")
rsq_wider
corrr::correlate(rsq_wider %>% select(-id), quiet = TRUE)
rsq_indiv_estimates %>%
mutate(wflow_id = reorder(wflow_id, .estimate)) %>%
ggplot(aes(x = wflow_id, y = .estimate, group = id, color = id)) +
geom_line(alpha = .5, linewidth = 1.25) +
theme(legend.position = "none")
#Chicago <- Chicago %>%
#  mutate(weekend = timeDate::isBizday(timeDate::as.timeDate(date)))
install.packages(c("bayesplot", "boot", "callr", "circlize", "cli", "data.table", "DBI", "dials", "digest", "DT", "estimability", "ggeffects", "ggiraph", "ggplot2", "glue", "igraph", "loo", "magick", "parallelly", "parsnip", "performance", "recipes", "rlang", "rvest", "segmented", "shape", "survival", "tidyr", "usethis", "workflows", "xfun", "yardstick"))
install.packages(c("bayesplot", "boot", "callr", "circlize", "cli", "data.table", "DBI", "dials", "digest", "DT", "estimability", "ggeffects", "ggiraph", "ggplot2", "glue", "igraph", "loo", "magick", "parallelly", "parsnip", "performance", "recipes", "rlang", "rvest", "segmented", "shape", "survival", "tidyr", "usethis", "workflows", "xfun", "yardstick"))
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(patchwork)
library(ggforce)
#library(doMC)
#registerDoMC(cores = parallel::detectCores())
tidymodels_prefer()
## -----------------------------------------------------------------------------
source("ames_snippets.R")
## -----------------------------------------------------------------------------
data(two_class_dat)
set.seed(91)
split <- initial_split(two_class_dat)
training_set <- training(split)
testing_set  <-  testing(split)
data_grid <- crossing(A = seq(0.4, 4, length = 200), B = seq(.14, 3.9, length = 200))
## -----------------------------------------------------------------------------
load("RData/search_examples.RData")
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(patchwork)
library(ggforce)
#library(doMC)
#registerDoMC(cores = parallel::detectCores())
tidymodels_prefer()
## -----------------------------------------------------------------------------
source("ames_snippets.R")
## -----------------------------------------------------------------------------
data(two_class_dat)
set.seed(91)
split <- initial_split(two_class_dat)
training_set <- training(split)
testing_set  <-  testing(split)
data_grid <- crossing(A = seq(0.4, 4, length = 200), B = seq(.14, 3.9, length = 200))
## -----------------------------------------------------------------------------
load("RData/search_examples.RData")
#| echo = FALSE,
#| fig.cap = "An example two-class classification data set with two predictors",
#| fig.alt = "An example two-class classification data set with two predictors. The two predictors have a moderate correlation and there is some locations of separation between the classes."
ggplot(training_set, aes(x = A, y = B, color = Class, pch = Class)) +
geom_point(alpha = 0.7) +
coord_equal()  +
labs(x = "Predictor A", y = "Predictor B", color = NULL, pch = NULL) +
scale_color_manual(values = c("#CC6677", "#88CCEE"))
library(tidymodels)
tidymodels_prefer()
llhood <- function(...) {
logistic_reg() %>%
set_engine("glm", ...) %>%
fit(Class ~ ., data = training_set) %>%
glance() %>%
select(logLik)
}
bind_rows(
llhood(),
llhood(family = binomial(link = "probit")),
llhood(family = binomial(link = "cloglog"))
) %>%
mutate(link = c("logit", "probit", "c-log-log"))  %>%
arrange(desc(logLik))
set.seed(1201)
rs <- vfold_cv(training_set, repeats = 10)
# Return the individual resampled performance estimates:
lloss <- function(...) {
perf_meas <- metric_set(roc_auc, mn_log_loss)
logistic_reg() %>%
set_engine("glm", ...) %>%
fit_resamples(Class ~ A + B, rs, metrics = perf_meas) %>%
collect_metrics(summarize = FALSE) %>%
select(id, id2, .metric, .estimate)
}
resampled_res <-
bind_rows(
lloss()                                    %>% mutate(model = "logistic"),
lloss(family = binomial(link = "probit"))  %>% mutate(model = "probit"),
lloss(family = binomial(link = "cloglog")) %>% mutate(model = "c-log-log")
) %>%
# Convert log-loss to log-likelihood:
mutate(.estimate = ifelse(.metric == "mn_log_loss", -.estimate, .estimate)) %>%
group_by(model, .metric) %>%
summarize(
mean = mean(.estimate, na.rm = TRUE),
std_err = sd(.estimate, na.rm = TRUE) / sqrt(n()),
.groups = "drop"
)
resampled_res %>%
filter(.metric == "mn_log_loss") %>%
ggplot(aes(x = mean, y = model)) +
geom_point() +
geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err),
width = .1) +
labs(y = NULL, x = "log-likelihood")
#| echo = FALSE,
#| fig.height = 3,
#| fig.cap = "Means and approximate 90% confidence intervals for the resampled binomial log-likelihood with three different link functions",
#| fig.alt = "Means and approximate 90% confidence intervals for the resampled binomial log-likelihood with three different link functions. The logit link has the largest value, followed by the probit link. The complementary log log link has far lower values."
#| echo = FALSE,
#| fig.height = 3,
#| fig.cap = "Means and approximate 90% confidence intervals for the resampled area under the ROC curve with three different link functions",
#| fig.alt = "Means and approximate 90% confidence intervals for the resampled area under the ROC curve with three different link functions. The logit link has the largest value, followed by the probit link. The confidence intervals show a large amount of overlap between the two methods."
resampled_res %>%
filter(.metric == "roc_auc") %>%
ggplot(aes(x = mean, y = model)) +
geom_point() +
geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean+ 1.64 * std_err),
width = .1) +
labs(y = NULL, x = "area under the ROC curve")
#| echo = FALSE,
#| fig.cap = "The linear class boundary fits for three link functions",
#| fig.alt = "The linear class boundary fits for three link functions. The lines have very similar slopes with the complementary log log having a slightly different intercept than the other two links."
logit_pred <-
logistic_reg() %>%
set_engine("glm") %>%
fit(Class ~ A + B, data = training_set) %>%
predict(data_grid, type = "prob") %>%
bind_cols(data_grid) %>%
mutate(link = "logit")
probit_pred <-
logistic_reg() %>%
set_engine("glm", family = binomial(link = "probit")) %>%
fit(Class ~ A + B, data = training_set) %>%
predict(data_grid, type = "prob") %>%
bind_cols(data_grid) %>%
mutate(link = "probit")
cloglog_pred <-
logistic_reg() %>%
set_engine("glm", family = binomial(link = "cloglog")) %>%
fit(Class ~ A + B, data = training_set) %>%
predict(data_grid, type = "prob") %>%
bind_cols(data_grid) %>%
mutate(link = "c-log-log")
link_grids <-
bind_rows(logit_pred, probit_pred, cloglog_pred) %>%
mutate(link = factor(link, levels = c("logit", "probit", "c-log-log")))
link_grids %>%
ggplot(aes(x = A, y = B)) +
geom_point(data = testing_set, aes(color = Class, pch = Class),
alpha = 0.7, show.legend = FALSE) +
geom_contour(aes( z = .pred_Class1, lty = link), breaks = 0.5, color = "black") +
scale_color_manual(values = c("#CC6677", "#88CCEE")) +
coord_equal() +
labs(x = "Predictor A", y = "Predictor B")
#| echo = FALSE,
#| fig.cap = "The linear class boundary fits for three link functions",
#| fig.alt = "The linear class boundary fits for three link functions. The lines have very similar slopes with the complementary log log having a slightly different intercept than the other two links."
logit_pred <-
logistic_reg() %>%
set_engine("glm") %>%
fit(Class ~ A + B, data = training_set) %>%
predict(data_grid, type = "prob") %>%
bind_cols(data_grid) %>%
mutate(link = "logit")
probit_pred <-
logistic_reg() %>%
set_engine("glm", family = binomial(link = "probit")) %>%
fit(Class ~ A + B, data = training_set) %>%
predict(data_grid, type = "prob") %>%
bind_cols(data_grid) %>%
mutate(link = "probit")
cloglog_pred <-
logistic_reg() %>%
set_engine("glm", family = binomial(link = "cloglog")) %>%
fit(Class ~ A + B, data = training_set) %>%
predict(data_grid, type = "prob") %>%
bind_cols(data_grid) %>%
mutate(link = "c-log-log")
link_grids <-
bind_rows(logit_pred, probit_pred, cloglog_pred) %>%
mutate(link = factor(link, levels = c("logit", "probit", "c-log-log")))
link_grids %>%
ggplot(aes(x = A, y = B)) +
geom_point(data = testing_set, aes(color = Class, pch = Class),
alpha = 0.7, show.legend = FALSE) +
geom_contour(aes( z = .pred_Class1, lty = link), breaks = 0.5, color = "black") +
scale_color_manual(values = c("#CC6677", "#88CCEE")) +
coord_equal() +
labs(x = "Predictor A", y = "Predictor B")
two_class_rec <-
recipe(Class ~ ., data = two_class_dat) %>%
step_normalize(all_numeric_predictors())
mlp_mod <-
mlp(hidden_units = tune(), epochs = 1000) %>%
set_engine("nnet") %>%
set_mode("classification")
mlp_wflow <-
workflow() %>%
add_recipe(two_class_rec) %>%
add_model(mlp_mod)
mlp_res <-
tibble(
hidden_units = 1:20,
train = NA_real_,
test = NA_real_,
model = vector(mode = "list", length = 20)
)
for(i in 1:nrow(mlp_res)) {
set.seed(27)
tmp_mod <-
mlp_wflow %>% finalize_workflow(mlp_res %>% slice(i) %>% select(hidden_units)) %>%
fit(training_set)
mlp_res$train[i] <-
roc_auc_vec(training_set$Class, predict(tmp_mod, training_set, type = "prob")$.pred_Class1)
mlp_res$test[i]  <-
roc_auc_vec(testing_set$Class, predict(tmp_mod, testing_set, type = "prob")$.pred_Class1)
mlp_res$model[[i]] <- tmp_mod
}
#| echo = FALSE,
#| fig.height = 8,
#| fig.cap = "Class boundaries for three models with increasing numbers of hidden units. The boundaries are fit on the training set and shown for the training and test sets.",
#| fig.alt = "Class boundaries for three models with increasing numbers of hidden units. The boundaries are fit on the training set and shown for the training and test sets. After a single hidden unit, the boundaries become wildly complex. The test set plots show that the more complex models do not conform to the data that was not used to fit the model."
te_plot <-
mlp_res %>%
slice(c(1, 4, 20)) %>%
mutate(
probs = map(model, ~ bind_cols(data_grid, predict(.x, data_grid, type = "prob")))
) %>%
dplyr::select(hidden_units, probs) %>%
unnest(cols = c(probs)) %>%
mutate(
label = paste(format(hidden_units), "units"),
label = ifelse(label == " 1 units", " 1 unit", label)
) %>%
ggplot(aes(x = A, y = B)) +
geom_point(data = testing_set, aes(color = Class, pch = Class),
alpha = 0.5, show.legend = FALSE) +
geom_contour(aes( z = .pred_Class1), breaks = 0.5, color = "black") +
scale_color_manual(values = c("#CC6677", "#88CCEE")) +
facet_wrap(~ label, nrow = 1) +
coord_equal() +
ggtitle("Test Set") +
labs(x = "Predictor A", y = "Predictor B")
tr_plot <-
mlp_res %>%
slice(c(1, 4, 20)) %>%
mutate(
probs = map(model, ~ bind_cols(data_grid, predict(.x, data_grid, type = "prob")))
) %>%
dplyr::select(hidden_units, probs) %>%
unnest(cols = c(probs)) %>%
mutate(
label = paste(format(hidden_units), "units"),
label = ifelse(label == " 1 units", " 1 unit", label)
) %>%
ggplot(aes(x = A, y = B)) +
geom_point(data = training_set, aes(color = Class, pch = Class),
alpha = 0.5, show.legend = FALSE) +
geom_contour(aes( z = .pred_Class1), breaks = 0.5, color = "black") +
scale_color_manual(values = c("#CC6677", "#88CCEE")) +
facet_wrap(~ label, nrow = 1) +
coord_equal() +
ggtitle("Training Set") +
labs(x = "Predictor A", y = "Predictor B")
tr_plot / te_plot
#| echo = FALSE,
#| fig.cap = "Examples of pre-defined grid tuning and an iterative search method. The lines represent contours of a performance metric; it is best in the upper-right-hand side of the plot.",
#| fig.alt = "Examples of pre-defined grid tuning and an iterative search method. The lines represent contours of some performance metric that is best in the upper-right-hand side of the plot. The grid search shows points that cover the space well and has one point near the optimum. The iterative search method has many more points and meanders to the optimum where many points zero in on the best value."
grid_plot <-
ggplot(sfd_grid, aes(x = x, y = y)) +
geom_point() +
lims(x = 0:1, y = 0:1) +
labs(x = "Parameter 1", y = "Parameter 2", title = "Space-Filling Grid") +
geom_contour(data = grid_contours,
aes(z = obj),
alpha = .3,
bins = 12) +
coord_equal() +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank())
search_plot <-
ggplot(nm_res, aes(x = x, y = y)) +
geom_point(size = .7)  +
lims(x = 0:1, y = 0:1) +
labs(x = "Parameter 1", y = "Parameter 2", title = "Global Search") +
coord_equal()  +
geom_contour(data = grid_contours,
aes(x = x, y = y, z = obj),
alpha = .3,
bins = 12) +
theme_bw() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank())
grid_plot + search_plot
rand_forest(trees = 2000, min_n = 10) %>%                   # <- main arguments
set_engine("ranger", regularization.factor = 0.5)         # <- engine-specific
neural_net_spec <-
mlp(hidden_units = tune()) %>%
set_mode("regression") %>%
set_engine("keras")
tune()
extract_parameter_set_dials(neural_net_spec)
ames_rec <-
recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +
Latitude + Longitude, data = ames_train)  %>%
step_log(Gr_Liv_Area, base = 10) %>%
step_other(Neighborhood, threshold = tune()) %>%
step_dummy(all_nominal_predictors()) %>%
step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>%
step_ns(Longitude, deg_free = tune("longitude df")) %>%
step_ns(Latitude,  deg_free = tune("latitude df"))
recipes_param <- extract_parameter_set_dials(ames_rec)
recipes_param
wflow_param <-
workflow() %>%
add_recipe(ames_rec) %>%
add_model(neural_net_spec) %>%
extract_parameter_set_dials()
wflow_param
hidden_units()
threshold()
spline_degree()
# identify the parameter using the id value:
wflow_param %>% extract_parameter_dials("threshold")
extract_parameter_set_dials(ames_rec) %>%
update(threshold = threshold(c(0.8, 1.0)))
rf_spec <-
rand_forest(mtry = tune()) %>%
set_engine("ranger", regularization.factor = tune("regularization")) %>%
set_mode("regression")
rf_param <- extract_parameter_set_dials(rf_spec)
rf_param
rf_param %>%
update(mtry = mtry(c(1, 70)))
pca_rec <-
recipe(Sale_Price ~ ., data = ames_train) %>%
# Select the square-footage predictors and extract their PCA components:
step_normalize(contains("SF")) %>%
# Select the number of components needed to capture 95% of
# the variance in the predictors.
step_pca(contains("SF"), threshold = .95)
updated_param <-
workflow() %>%
add_model(rf_spec) %>%
add_recipe(pca_rec) %>%
extract_parameter_set_dials() %>%
finalize(ames_train)
updated_param
updated_param %>% extract_parameter_dials("mtry")
rf_param
regularization_factor()
penalty()
# correct method to have penalty values between 0.1 and 1.0
penalty(c(-1, 0)) %>% value_sample(1000) %>% summary()
# incorrect:
penalty(c(0.1, 1.0)) %>% value_sample(1000) %>% summary()
penalty(trans = NULL, range = 10^c(-10, 0))
