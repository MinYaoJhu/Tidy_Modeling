chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred, col = weekday)) +
geom_point(alpha = 0.5) +
labs(y = "Predicted ridership", x = "Ridership") +
coord_obs_pred()
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred, col = weekday)) +
geom_point(alpha = 0.5) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
coord_equal()
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred, col = weekday)) +
geom_point(alpha = 0.5) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = "lightblue") +
coord_equal()
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred, col = weekday)) +
geom_point(alpha = 0.5) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = "green") +
coord_equal()
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred, col = weekday)) +
geom_point(alpha = 0.5) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = "black") +
coord_equal()
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred, col = weekday)) +
geom_point(alpha = 0.5) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = "grey") +
coord_equal()
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred, col = weekday)) +
geom_point(alpha = 0.5) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
geom_smooth(method = lm, formula = y ~ x) +
coord_equal()
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred, col = weekday)) +
geom_point(alpha = 0.5) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
geom_smooth(method = lm, formula = y ~ x, se = FALSE) +
coord_equal()
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred, col = weekday)) +
geom_point(alpha = 0.5) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = "lightblue") +
coord_equal()
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred, col = weekday)) +
geom_point(alpha = 0.5) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
geom_smooth(method = lm, formula = y ~ x, color = "lightblue") +
coord_equal()
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred, col = weekday)) +
geom_point(alpha = 0.5) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
geom_smooth(method = lm, formula = y ~ x, color = "blue") +
coord_equal()
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred)) +
geom_point(alpha = 0.5, col = weekday) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
geom_smooth(method = lm, formula = y ~ x) +
coord_equal()
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred)) +
geom_point(alpha = 0.5, col = aes(weekday)) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
geom_smooth(method = lm, formula = y ~ x) +
coord_equal()
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred)) +
geom_point(alpha = 0.5, col = aes(~weekday)) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
geom_smooth(method = lm, formula = y ~ x) +
coord_equal()
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred, col = weekday)) +
geom_point(alpha = 0.5) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
geom_smooth(method = lm, formula = y ~ x, color = "blue") +
coord_equal()
#?Chicago
library(tidymodels)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(workflowsets)
str(Chicago)
stations
y_hist <-
ggplot(Chicago, aes(ridership)) +
geom_histogram(binwidth = .7, fill = "blue", col = "blue", alpha = .5) +
xlab("Ridership (x1000 riders)")
y_hist
Chicago_week <- Chicago %>%
mutate(dow = as.character(wday(ymd(date), label = T))) %>%
mutate(weekday = as.factor(ifelse(dow %in% c("Sat","Sun"), "Weekend", "Weekday")))
Chicago_week
fig_Chicago_week <-
ggplot(Chicago_week, aes(ridership, fill = weekday, col = weekday)) +
facet_wrap( ~ weekday, nrow = 2, scales = "free_y") +
geom_histogram(binwidth = .7, alpha = .5) +
xlab("Ridership (x1000 riders)")
fig_Chicago_week
set.seed(010324)
Chicago_split <- initial_split(Chicago_week, prop = 0.80, strata = ridership)
Chicago_train <- training(Chicago_split)
Chicago_test  <-  testing(Chicago_split)
dim(Chicago_week)
dim(Chicago_train)
dim(Chicago_test)
lm_model <- linear_reg() %>% set_engine("lm")
weather <- list(
temp_min = ridership ~ temp_min + weekday,
temp = ridership ~ temp + weekday,
temp_max = ridership ~ temp_max + weekday,
temp_change = ridership ~ temp_change + weekday,
percip = ridership ~ percip + weekday,
percip_max = ridership ~ percip_max + weekday
)
weather_models <- workflow_set(preproc = weather,
models = list(lm = lm_model))
weather_models
weather_models$info[[1]]
extract_workflow(weather_models, id = "temp_lm")
weather_models <-
weather_models %>%
mutate(fit = map(info, ~ fit(.x$workflow[[1]], Chicago_train)))
weather_models
weather_models %>%
mutate(tidy=map(fit, tidy)) %>%
unnest(tidy) %>%
filter(str_detect(term, "temp|percip")) %>%
arrange(p.value)
recipe(ridership ~ ., data = Chicago_train) %>%
add_role(Austin:California, new_role = "station") %>%
add_role(temp_min:weather_storm, new_role = "weather")
recipe(ridership ~ ., data = Chicago_train) %>%
add_role(Austin:California, new_role = "station") %>%
add_role(temp_min:weather_storm, new_role = "weather") %>%
step_normalize(has_role("station")) %>%
step_normalize(has_role("weather"))
recipe(ridership ~ ., data = Chicago_train) %>%
add_role(Austin:California, new_role = "station") %>%
add_role(temp_min:weather_storm, new_role = "weather") %>%
step_normalize(has_role("station")) %>%
step_normalize(has_role("weather")) %>%
step_pca(has_role("station"), threshold = .75, prefix = "station_PC", id = "station_pca") %>%
step_pca(has_role("weather"), threshold = .75, prefix = "weather_PC", id = "weather_pca")
Chicago_rec <-
recipe(ridership ~ ., data = Chicago_train) %>%
add_role(Austin:California, new_role = "station") %>%
add_role(temp_min:weather_storm, new_role = "weather") %>%
step_normalize(has_role("station")) %>%
step_normalize(has_role("weather")) %>%
step_pca(has_role("station"), threshold = .75, prefix = "station_PC", id = "station_pca") %>%
step_pca(has_role("weather"), threshold = .75, prefix = "weather_PC", id = "weather_pca")
Chicago_rec
tidy(Chicago_rec)
lm_wflow <-
workflow() %>%
add_model(lm_model) %>%
add_recipe(Chicago_rec)
lm_fit <- fit(lm_wflow, Chicago_train)
tidy(lm_fit)
tidy(lm_fit) %>% select(term, p.value)
str(Chicago_test)
Chicago_test_nor <- Chicago_test %>% select(-ridership)
str(Chicago_test_nor)
Chicago_test_onlyr <- Chicago_test %>% select(ridership,weekday)
str(Chicago_test_onlyr)
predicted_r <- predict(lm_fit, Chicago_test_nor)
predicted_r
chicago_test_predictions <- bind_cols(predicted_r, Chicago_test_onlyr)
chicago_test_predictions
chicago_test_predictions |>
ggplot(aes(x = ridership, y = .pred, col = weekday)) +
geom_point(alpha = 0.5) +
labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
geom_smooth(method = lm, formula = y ~ x, color = "blue") +
coord_equal()
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(kableExtra)
tidymodels_prefer()
source("ames_snippets.R")
load("RData/lm_fit.RData")
data(ad_data)
set.seed(245)
ad_folds <- vfold_cv(ad_data, repeats = 5)
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(kableExtra)
tidymodels_prefer()
source("ames_snippets.R")
load("RData/lm_fit.RData")
data(ad_data)
set.seed(245)
ad_folds <- vfold_cv(ad_data, repeats = 5)
#| fig.cap = "Observed versus predicted values for models that are optimized using the RMSE compared to the coefficient of determination",
#| fig.alt = "Scatter plots of numeric observed versus predicted values for models that are optimized using the RMSE and the coefficient of determination. The former results in results that are close to the 45 degree line of identity while the latter shows results with a tight linear correlation but falls well off of the line of identity."
set.seed(234)
n <- 200
obs <- runif(n, min = 2, max = 20)
reg_ex <-
tibble(
observed = c(obs, obs),
predicted = c(obs + rnorm(n, sd = 1.5), 5 + .5 * obs + rnorm(n, sd = .5)),
approach = rep(c("RMSE optimized", "R^2 optimized"), each = n)
) %>%
mutate(approach = factor(
approach,
levels = c("RMSE optimized", "R^2 optimized"),
labels = c(expression(RMSE ~ optimized), expression(italic(R^2) ~ optimized)))
)
ggplot(reg_ex, aes(x = observed, y = predicted)) +
geom_abline(lty = 2) +
geom_point(alpha = 0.5) +
coord_obs_pred() +
facet_wrap(~ approach, labeller = "label_parsed")
ad_mod <- logistic_reg() %>% set_engine("glm")
full_model_fit <-
ad_mod %>%
fit(Class ~ (Genotype + male + age)^3, data = ad_data)
full_model_fit %>% extract_fit_engine()
two_way_fit <-
ad_mod %>%
fit(Class ~ (Genotype + male + age)^2, data = ad_data)
three_factor_test <-
anova(
full_model_fit %>% extract_fit_engine(),
two_way_fit %>% extract_fit_engine(),
test = "LRT"
)
main_effects_fit <-
ad_mod %>%
fit(Class ~ Genotype + male + age, data = ad_data)
two_factor_test <-
anova(
two_way_fit %>% extract_fit_engine(),
main_effects_fit %>% extract_fit_engine(),
test = "LRT"
)
two_factor_rs <-
ad_mod %>%
fit_resamples(Class ~ (Genotype + male + age)^2, ad_folds)
two_factor_res <-
collect_metrics(two_factor_rs) %>%
filter(.metric == "accuracy") %>%
pull(mean)
ames_test_res <- predict(lm_fit, new_data = ames_test %>% select(-Sale_Price))
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(kableExtra)
tidymodels_prefer()
source("ames_snippets.R")
load("RData/lm_fit.RData")
data(ad_data)
set.seed(245)
ad_folds <- vfold_cv(ad_data, repeats = 5)
#| fig.cap = "Observed versus predicted values for models that are optimized using the RMSE compared to the coefficient of determination",
#| fig.alt = "Scatter plots of numeric observed versus predicted values for models that are optimized using the RMSE and the coefficient of determination. The former results in results that are close to the 45 degree line of identity while the latter shows results with a tight linear correlation but falls well off of the line of identity."
set.seed(234)
n <- 200
obs <- runif(n, min = 2, max = 20)
reg_ex <-
tibble(
observed = c(obs, obs),
predicted = c(obs + rnorm(n, sd = 1.5), 5 + .5 * obs + rnorm(n, sd = .5)),
approach = rep(c("RMSE optimized", "R^2 optimized"), each = n)
) %>%
mutate(approach = factor(
approach,
levels = c("RMSE optimized", "R^2 optimized"),
labels = c(expression(RMSE ~ optimized), expression(italic(R^2) ~ optimized)))
)
ggplot(reg_ex, aes(x = observed, y = predicted)) +
geom_abline(lty = 2) +
geom_point(alpha = 0.5) +
coord_obs_pred() +
facet_wrap(~ approach, labeller = "label_parsed")
ad_mod <- logistic_reg() %>% set_engine("glm")
full_model_fit <-
ad_mod %>%
fit(Class ~ (Genotype + male + age)^3, data = ad_data)
full_model_fit %>% extract_fit_engine()
two_way_fit <-
ad_mod %>%
fit(Class ~ (Genotype + male + age)^2, data = ad_data)
three_factor_test <-
anova(
full_model_fit %>% extract_fit_engine(),
two_way_fit %>% extract_fit_engine(),
test = "LRT"
)
main_effects_fit <-
ad_mod %>%
fit(Class ~ Genotype + male + age, data = ad_data)
two_factor_test <-
anova(
two_way_fit %>% extract_fit_engine(),
main_effects_fit %>% extract_fit_engine(),
test = "LRT"
)
two_factor_rs <-
ad_mod %>%
fit_resamples(Class ~ (Genotype + male + age)^2, ad_folds)
two_factor_res <-
collect_metrics(two_factor_rs) %>%
filter(.metric == "accuracy") %>%
pull(mean)
ames_test_res <- predict(lm_fit, new_data = ames_test %>% select(-Sale_Price))
library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))
set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)
ames_rec <-
recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type +
Latitude + Longitude, data = ames_train) %>%
step_log(Gr_Liv_Area, base = 10) %>%
step_other(Neighborhood, threshold = 0.01) %>%
step_dummy(all_nominal_predictors()) %>%
step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>%
step_ns(Latitude, Longitude, deg_free = 20)
lm_model <- linear_reg() %>% set_engine("lm")
lm_wflow <-
workflow() %>%
add_model(lm_model) %>%
add_recipe(ames_rec)
lm_fit <- fit(lm_wflow, ames_train)
ames_test_res <- predict(lm_fit, new_data = ames_test %>% select(-Sale_Price))
ames_test_res
ames_test_res <- bind_cols(ames_test_res, ames_test %>% select(Sale_Price))
ames_test_res
ggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) +
# Create a diagonal line:
geom_abline(lty = 2) +
geom_point(alpha = 0.5) +
labs(y = "Predicted Sale Price (log10)", x = "Sale Price (log10)") +
# Scale and size the x- and y-axis uniformly:
coord_obs_pred()
#| echo = FALSE,
#| fig.cap = "Observed versus predicted values for an Ames regression model, with log-10 units on both axes",
#| fig.alt = "Scatter plots of numeric observed versus predicted values for an Ames regression model. Both axes use log-10 units. The model shows good concordance with some poorly fitting points at high and low prices."
rmse(ames_test_res, truth = Sale_Price, estimate = .pred)
ames_metrics <- metric_set(rmse, rsq, mae)
ames_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)
data(two_class_example)
tibble(two_class_example)
# A confusion matrix:
conf_mat(two_class_example, truth = truth, estimate = predicted)
# Accuracy:
accuracy(two_class_example, truth, predicted)
# Matthews correlation coefficient:
mcc(two_class_example, truth, predicted)
# F1 metric:
f_meas(two_class_example, truth, predicted)
# Combining these three classification metrics together
classification_metrics <- metric_set(accuracy, mcc, f_meas)
classification_metrics(two_class_example, truth = truth, estimate = predicted)
f_meas(two_class_example, truth, predicted, event_level = "second")
two_class_curve <- roc_curve(two_class_example, truth, Class1)
two_class_curve
roc_auc(two_class_example, truth, Class1)
data(hpc_cv)
tibble(hpc_cv)
library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(kableExtra)
tidymodels_prefer()
source("ames_snippets.R")
load("RData/lm_fit.RData")
data(ad_data)
set.seed(245)
ad_folds <- vfold_cv(ad_data, repeats = 5)
#| fig.cap = "Observed versus predicted values for models that are optimized using the RMSE compared to the coefficient of determination",
#| fig.alt = "Scatter plots of numeric observed versus predicted values for models that are optimized using the RMSE and the coefficient of determination. The former results in results that are close to the 45 degree line of identity while the latter shows results with a tight linear correlation but falls well off of the line of identity."
set.seed(234)
n <- 200
obs <- runif(n, min = 2, max = 20)
reg_ex <-
tibble(
observed = c(obs, obs),
predicted = c(obs + rnorm(n, sd = 1.5), 5 + .5 * obs + rnorm(n, sd = .5)),
approach = rep(c("RMSE optimized", "R^2 optimized"), each = n)
) %>%
mutate(approach = factor(
approach,
levels = c("RMSE optimized", "R^2 optimized"),
labels = c(expression(RMSE ~ optimized), expression(italic(R^2) ~ optimized)))
)
ggplot(reg_ex, aes(x = observed, y = predicted)) +
geom_abline(lty = 2) +
geom_point(alpha = 0.5) +
coord_obs_pred() +
facet_wrap(~ approach, labeller = "label_parsed")
ad_mod <- logistic_reg() %>% set_engine("glm")
full_model_fit <-
ad_mod %>%
fit(Class ~ (Genotype + male + age)^3, data = ad_data)
full_model_fit %>% extract_fit_engine()
two_way_fit <-
ad_mod %>%
fit(Class ~ (Genotype + male + age)^2, data = ad_data)
three_factor_test <-
anova(
full_model_fit %>% extract_fit_engine(),
two_way_fit %>% extract_fit_engine(),
test = "LRT"
)
main_effects_fit <-
ad_mod %>%
fit(Class ~ Genotype + male + age, data = ad_data)
two_factor_test <-
anova(
two_way_fit %>% extract_fit_engine(),
main_effects_fit %>% extract_fit_engine(),
test = "LRT"
)
two_factor_rs <-
ad_mod %>%
fit_resamples(Class ~ (Genotype + male + age)^2, ad_folds)
two_factor_res <-
collect_metrics(two_factor_rs) %>%
filter(.metric == "accuracy") %>%
pull(mean)
data(hpc_cv)
tibble(hpc_cv)
accuracy(hpc_cv, obs, pred)
mcc(hpc_cv, obs, pred)
class_totals <-
count(hpc_cv, obs, name = "totals") %>%
mutate(class_wts = totals / sum(totals))
class_totals
cell_counts <-
hpc_cv %>%
group_by(obs, pred) %>%
count() %>%
ungroup()
cell_counts
cell_counts %>%
filter(obs == pred)
cell_counts %>%
filter(obs == pred) %>%
full_join(class_totals, by = "obs")
cell_counts %>%
filter(obs == pred) %>%
full_join(class_totals, by = "obs") %>%
mutate(sens = n / totals)
hpc_cv %>%
group_by(obs, pred)
hpc_cv %>%
group_by(obs, pred) %>%
count()
one_versus_all <-
cell_counts %>%
filter(obs == pred) %>%
full_join(class_totals, by = "obs") %>%
mutate(sens = n / totals)
one_versus_all
class_totals <-
count(hpc_cv, obs, name = "totals") %>%
mutate(class_wts = totals / sum(totals))
class_totals
cell_counts <-
hpc_cv %>%
group_by(obs, pred) %>%
count() %>%
ungroup()
# Compute the four sensitivities using 1-vs-all
one_versus_all <-
cell_counts %>%
filter(obs == pred) %>%
full_join(class_totals, by = "obs") %>%
mutate(sens = n / totals)
one_versus_all
# Three different estimates:
one_versus_all %>%
summarize(
macro = mean(sens),
macro_wts = weighted.mean(sens, class_wts),
micro = sum(n) / sum(totals)
)
sensitivity(hpc_cv, obs, pred, estimator = "macro")
sensitivity(hpc_cv, obs, pred, estimator = "macro_weighted")
sensitivity(hpc_cv, obs, pred, estimator = "micro")
roc_auc(hpc_cv, obs, VF, F, M, L)
roc_auc(hpc_cv, obs, VF, F, M, L, estimator = "macro_weighted")
hpc_cv %>%
group_by(Resample) %>%
accuracy(obs, pred)
# Four 1-vs-all ROC curves for each fold
hpc_cv %>%
group_by(Resample) %>%
roc_curve(obs, VF, F, M, L) %>%
autoplot()
#| echo = FALSE,
#| fig.cap = "Resampled ROC curves for each of the four outcome classes",
#| fig.alt = "Resampled ROC curves for each of the four outcome classes. There are four panels for classes VF, F, M, and L. Each panel contains ten ROC curves for each of the resampled data sets."
