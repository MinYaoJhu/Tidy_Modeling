---
title: "Practice-chapter-7-8.Rmd"
author: "Min-Yao Jhu"
date: "2024-01-07"
output: 
  html_document: 
    keep_md: yes
---

We will practice using the Chicago data set on public transit ridership.  You can access it with:

```{r, eval=FALSE}
library(modeldata)
data("Chicago")
Chicago
```

Read about it with `?Chicago`; read more about it in [Kuhn and Johnson](https://bookdown.org/max/FES/chicago-intro.html)

```{r}
#?Chicago
```

```{r}
library(tidymodels)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(workflowsets)
```


## 1.  Explore the data

1. Make a histogram of ridership.  

```{r}
str(Chicago)
stations
```

> Ridership is measured by the number of entries into a station across all turnstiles, and the number of daily riders across stations during this time period varied considerably, ranging between 0 and 36,323 per day. 

> For ease of presentation, ridership will be shown and analyzed in units of thousands of riders.

```{r}
y_hist <- 
  ggplot(Chicago, aes(ridership)) +   
  geom_histogram(binwidth = .7, fill = "blue", col = "blue", alpha = .5) +
  xlab("Ridership (x1000 riders)")

y_hist
```

2. What might be causing the two peaks.  Is there a predictor variable that can account for this (or that can be used to make a new variable to account for it)?

I am not suggesting that you do regressions or plots on all variables at this time, rather that you think about what might have this kind of impact.

If you need to make a new predictor variable, go ahead.

```{r}
Chicago_week <- Chicago %>%
  mutate(dow = as.character(wday(ymd(date), label = T))) %>% 
  mutate(weekday = as.factor(ifelse(dow %in% c("Sat","Sun"), "Weekend", "Weekday")))
Chicago_week

fig_Chicago_week <- 
  ggplot(Chicago_week, aes(ridership, fill = weekday, col = weekday)) + 
  facet_wrap( ~ weekday, nrow = 2, scales = "free_y") +
  geom_histogram(binwidth = .7, alpha = .5) +
  xlab("Ridership (x1000 riders)")
fig_Chicago_week
```

> A reasonable explanation for this would be that ridership is different for weekdays than for weekends. 

> This is important and necessary for explaining ridership and should be included in a model.

## 2. Training and Test

Make an 80/20 train/test split.  Do you need to stratify over anything?  

So that we are working on the same split, use `set.seed(010324)` in you code chunk

```{r}
set.seed(010324)

Chicago_split <- initial_split(Chicago_week, prop = 0.80, strata = ridership)
Chicago_train <- training(Chicago_split)
Chicago_test  <-  testing(Chicago_split)
```

```{r}
dim(Chicago_week)
dim(Chicago_train)
dim(Chicago_test)
```

## 3. Workflow set

Let's compare the effectiveness  of the temp and percip [sic] predictors.  

### 3A 

Use a workflow set (see chapter 7) to fit six models, each of which has your predictor from Q1 along with one of the following variables:

`temp_min`, `temp`, `temp_max`, `temp_change`, `percip`, `percip_max`

The formula for one of these would be something like `ridership ~ temp_min + Q1_predictor`.

```{r}
lm_model <- linear_reg() %>% set_engine("lm")
```

```{r}
weather <- list(
  temp_min = ridership ~ temp_min + weekday,
  temp = ridership ~ temp + weekday,
  temp_max = ridership ~ temp_max + weekday,
  temp_change = ridership ~ temp_change + weekday,
  percip = ridership ~ percip + weekday,
  percip_max = ridership ~ percip_max + weekday
)
```

```{r}
weather_models <- workflow_set(preproc = weather, 
                               models = list(lm = lm_model))
weather_models
```

```{r}
weather_models$info[[1]]
```

```{r}
extract_workflow(weather_models, id = "temp_lm")
```

```{r}
weather_models <-
   weather_models %>%
   mutate(fit = map(info, ~ fit(.x$workflow[[1]], Chicago_train)))
weather_models
```

### 3B 

Compare the model fits / predictors (this can be using any of the p-value of the predictor, R2, AIC, log-lik).  Don't worry about the test set, just compare goodness of fit when fit with the training set.

```{r}
weather_models %>%
  mutate(tidy=map(fit, tidy)) %>%
  unnest(tidy) %>%
  filter(str_detect(term, "temp|percip")) %>%
  arrange(p.value)
```


## 4 Recipes

### 4A

Create a workflow recipe does the following:

* normalizes all weather and station predictors
* creates a set of PCs for the weather-related predictors, keeping enough PCs to explain 75% of the variance in the weather variables
* creates a second set of PCs for the station-related predictors, keeping enough PCs to explaining 75% of the variance in these variables

Hint: `tidy()`, `prep()`, and `bake()` methods for recipes may be helpful in examining what you have done.  The help file on `recipe` is good to0.

Hint2: You can use various dplyr::select functions and regular expressions to avoid having to type out the variable names.  But as a fair-warning, it took me a lot longer to figure that out than it would have to just type then out.  (But next time it might be faster).  I can demo.

```{r}
recipe(ridership ~ ., data = Chicago_train) %>%
  add_role(Austin:California, new_role = "station") %>%
  add_role(temp_min:weather_storm, new_role = "weather")
```

```{r}
recipe(ridership ~ ., data = Chicago_train) %>%
  add_role(Austin:California, new_role = "station") %>%
  add_role(temp_min:weather_storm, new_role = "weather") %>%
  step_normalize(has_role("station")) %>%
  step_normalize(has_role("weather")) 
```
```{r}
recipe(ridership ~ ., data = Chicago_train) %>%
  add_role(Austin:California, new_role = "station") %>%
  add_role(temp_min:weather_storm, new_role = "weather") %>%
  step_normalize(has_role("station")) %>%
  step_normalize(has_role("weather")) %>%
  step_pca(has_role("station"), threshold = .75, prefix = "station_PC", id = "station_pca") %>%
  step_pca(has_role("weather"), threshold = .75, prefix = "weather_PC", id = "weather_pca")
```


```{r}
Chicago_rec <- 
  recipe(ridership ~ ., data = Chicago_train) %>%
  add_role(Austin:California, new_role = "station") %>%
  add_role(temp_min:weather_storm, new_role = "weather") %>%
  step_normalize(has_role("station")) %>%
  step_normalize(has_role("weather")) %>%
  step_pca(has_role("station"), threshold = .75, prefix = "station_PC", id = "station_pca") %>%
  step_pca(has_role("weather"), threshold = .75, prefix = "weather_PC", id = "weather_pca")

Chicago_rec
```

```{r}
tidy(Chicago_rec)
```


### 4B

Use the recipe from 4A to fit a linear regression of ridership on the new PCs and all remaining predictors (i.e. those not used in making the PCs).  Use the training data.

```{r}
lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(Chicago_rec)

lm_fit <- fit(lm_wflow, Chicago_train)

tidy(lm_fit)

tidy(lm_fit) %>% select(term, p.value)
```


### 4C

Use the fit from 4B to predict ridership in the test data.  Evaluate the predictions.

```{r}
str(Chicago_test)

Chicago_test_nor <- Chicago_test %>% select(-ridership)
str(Chicago_test_nor)

Chicago_test_onlyr <- Chicago_test %>% select(ridership,weekday)
str(Chicago_test_onlyr)
```

```{r}
predicted_r <- predict(lm_fit, Chicago_test_nor)
predicted_r
```

```{r}
chicago_test_predictions <- bind_cols(predicted_r, Chicago_test_onlyr)
chicago_test_predictions
```

```{r}
chicago_test_predictions |> 
  ggplot(aes(x = ridership, y = .pred, col = weekday)) + 
  geom_point(alpha = 0.5) +
  labs(y = "Predicted Ridership (x1000)", x = "Recorded Ridership (x1000)") +
  geom_smooth(method = lm, formula = y ~ x, color = "blue") +
  coord_equal()
```

